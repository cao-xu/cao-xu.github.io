<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CaoXu&#39;s Blog</title>
  
  <subtitle>想法、创意与实践</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.caoxu.club/"/>
  <updated>2020-06-15T02:17:44.322Z</updated>
  <id>http://www.caoxu.club/</id>
  
  <author>
    <name>CaoXu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>将ubuntu装到移动硬盘中即插即用</title>
    <link href="http://www.caoxu.club/2020/06/14/%E5%B0%86ubuntu%E8%A3%85%E5%88%B0%E7%A7%BB%E5%8A%A8%E7%A1%AC%E7%9B%98%E4%B8%AD%E5%8D%B3%E6%8F%92%E5%8D%B3%E7%94%A8/"/>
    <id>http://www.caoxu.club/2020/06/14/将ubuntu装到移动硬盘中即插即用/</id>
    <published>2020-06-14T12:35:24.000Z</published>
    <updated>2020-06-15T02:17:44.322Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>将ubuntu装进口袋里</p></blockquote><a id="more"></a><div class="note default">            <h2 id="一、Why-动机"><a href="#一、Why-动机" class="headerlink" title="一、Why? 动机"></a>一、Why? 动机</h2>          </div><p>安装ubuntu以往都是放到虚拟机里，但是想要上手一些深度学习的项目时，虚拟机的性能太低，不能充分利用自己机器的全部性能，怎么样才能充分利用本机的硬件资源，而又不必在本机硬盘中分配空间装双系统呢？</p><div class="note default">            <h2 id="二、如何想到的？"><a href="#二、如何想到的？" class="headerlink" title="二、如何想到的？"></a>二、如何想到的？</h2>          </div><p>因为之前听说过苹果电脑用户会使用SSD装<strong>win to go</strong>，为什么不能搞一个<strong>ubuntu to go</strong>?</p><p>废话不多说，开整~</p><div class="note default">            <h2 id="三、How-制作过程"><a href="#三、How-制作过程" class="headerlink" title="三、How? 制作过程"></a>三、How? 制作过程</h2>          </div><p><strong>以下内容参考博客:</strong></p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cubHVvZ3UuY29tLmNuL2Jsb2cvR0dBdXRvbWF0b24vcG9ydGFibGUtdWJ1bnR1LWJvb3RhYmxlLWluLVVFRkktYW5kLUJJT1Mj" title="https://www.luogu.com.cn/blog/GGAutomaton/portable-ubuntu-bootable-in-UEFI-and-BIOS#">把 Ubuntu 装到U盘里随身携带，并同时支持 BIOS 和 UEFI 启动<i class="fa fa-external-link"></i></span></p><p>因为有些老机器只支持 BIOS+MBR，有些新机器只支持 UEFI+GPT，所以需要同时支持 BIOS 和 UEFI。</p><p>此方法可能<strong>不适用于硬件资源虚拟化或者锁USB端口的机器。</strong></p><h3 id="3-1-准备"><a href="#3-1-准备" class="headerlink" title="3.1 准备"></a>3.1 准备</h3><ol><li><p>Vmware Workstation 15 PRO（装虚拟机）</p></li><li><p><strong>64位</strong> Ubuntu ISO 文件（UEFI 不支持 32 位系统）</p></li><li><p>推荐使用<strong>固态硬盘(USB 3.0)</strong>，也可以使用U盘安装体验，推荐64G以上，U盘速度直接影响使用体验，请务必使用能够长时间保持写入速度的U盘。</p></li></ol><h3 id="3-2-新建虚拟机并编辑配置"><a href="#3-2-新建虚拟机并编辑配置" class="headerlink" title="3.2 新建虚拟机并编辑配置"></a>3.2 新建虚拟机并编辑配置</h3><p>新建虚拟机 -&gt; 典型 -&gt; 稍后安装操作系统 -&gt; Linux(Ubuntu 64位) -&gt; 最大磁盘大小0.001GB -&gt; 调整虚拟机硬件配置如下</p><p>1 . CD/DVD</p><pre><code>选择 Ubuntu ISO 文件位置，启动时连接</code></pre><p>2 . USB控制器</p><pre><code>USB兼容性：USB 3.0</code></pre><p>3 . 添加硬盘</p><pre><code>计算机管理 -&gt; 磁盘管理</code></pre><img title="添加硬盘" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/添加硬盘.png"><p>记住自己的U盘的磁盘编号（比如我这里是磁盘3）</p><p>虚拟机设置 -&gt; 添加 -&gt; 硬盘 -&gt; SCSI -&gt; 使用物理磁盘 -&gt; 选择自己的U盘，并使用整个磁盘</p><img title="添加硬盘2" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/添加硬盘2.png"><p>4 . 设置 UEFI 启动</p><pre><code>虚拟机设置 -&gt; 选项 -&gt; 高级 -&gt; 固件类型 -&gt; UEFI</code></pre><img title="添加硬盘3" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/添加硬盘3.png"><h3 id="3-3-U盘分区"><a href="#3-3-U盘分区" class="headerlink" title="3.3 U盘分区"></a>3.3 U盘分区</h3><p>开启虚拟机。如果提示“物理磁盘已被使用”，请关闭所有可能使用这个U盘的程序，拔出U盘后重新插入。</p><p>成功设置 UEFI 后，开机应该看到这个画面。如果没有看到，请检查之前的步骤。</p><img title="分区1" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/分区1.png"><p>选择”Try Ubuntu without installing”。开机后启动 Gparted。</p><p>此时U盘应该是 sdb。如果不是，请把下面所有的 sdb 换成您自己的 sdX。</p><p>在 <strong>sdb</strong> 建立 GPT 分区表。</p><img title="分区2" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/分区2.png"><img title="分区3" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/分区3.png"><p><strong>第一个分区</strong>设置 <code>fat32</code>，用来拷贝文件。（因为win7似乎只能识别第一个分区）</p><p><strong>ESP分区（下图第三个）</strong>至少 100MB，设置 boot 和 esp 标识。如果是SSD的话，建议设为1GB。</p><p>在分区上<strong>右键</strong>可以修改标识(Manage Flags)。</p><p><span class="exturl" data-url="aHR0cDovL3Blb3BsZS51YnVudHUuY29tL35oYXBweWFyb24vdWRjLWNuL2thcm1pYy1odG1sL2NoMTBzMDIuaHRtbA==" title="http://people.ubuntu.com/~happyaron/udc-cn/karmic-html/ch10s02.html">Gparted使用教程<i class="fa fa-external-link"></i></span></p><p>下图仅供参考。<strong>如果不需要在 Windows 下拷贝文件，可以不设置第一个 fat32 分区。</strong></p><img title="分区4" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/分区4.png"><p><strong>注意：</strong></p><ol><li><p>虚拟机关闭后，win10 会提醒格式化所有无法识别的分区，千万不要手抖点确定。</p></li><li><p>分区设置请<strong>慎重考虑</strong>，之后更改会很麻烦。系统分区尽量划分得大一些。如果内存充足，可以不设置 swap。</p></li></ol><p>您是不是被最后的 1MB 折磨，强迫症发作了？</p><p>现在我们来解决这个问题，向这个空间添加 bios_grub 标记。</p><p>这里简单提及一下这个标记的作用。GPT 兼容 MBR，如果要让 grub 在 GPT 上使用 MBR 模式安装的话，需要设置这个标记。这个分区有以下三个特点：1MB 容量，不需要格式化，设置 bios_grub 标记。</p><p>在虚拟机中打开终端</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sudo gdisk /dev/sdb</span><br><span class="line">n</span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认编号，回车就行</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认开始位置，回车</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认结束位置，回车</span></span><br><span class="line">EF02 # EF02 就是 bios_grub</span><br><span class="line">p # 看到 Name 有 BIOS boot partition 就可以了</span><br><span class="line">w</span><br><span class="line">y</span><br></pre></td></tr></tbody></table></figure><p>最终是这样的。</p><img title="分区5" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/分区5.png"><h3 id="3-4-开始-UEFI-安装"><a href="#3-4-开始-UEFI-安装" class="headerlink" title="3.4 开始 UEFI 安装"></a>3.4 开始 UEFI 安装</h3><p>双击桌面上的”Install Ubuntu”。</p><p><span class="exturl" data-url="aHR0cHM6Ly93d3cuc3lzZ2Vlay5jbi9pbnN0YWxsLXVidW50dS0xOC0wNC1sdHMv" title="https://www.sysgeek.cn/install-ubuntu-18-04-lts/">如果您没有安装 Ubuntu 的经历，点击这里了解流程。<i class="fa fa-external-link"></i></span></p><p>安装过程中下载更新很慢，建议断网。其他设置默认，直到这一步。</p><img title="UEFI安装1" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/UEFI安装1.png"><img title="UEFI安装2" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/UEFI安装2.png"><p>把 ext4 挂载到 /。也可以按照自己的需要来设置。</p><p><strong>启动引导器</strong>一定要装在 /dev/sdb 上，不能装在别的硬盘（比如 sda），也不能装在单个分区上。</p><p>之后设置用户名和密码。</p><p>安装好后，试一下能不能 UEFI 启动。</p><p><strong>再次提醒：</strong><br>关闭虚拟机后，如果您的电脑突然多了一堆盘符，提醒格式化，<strong>千万不要点击确定</strong>。可以使用 DiskGenius，在不明分区上右键，删除驱动器号（盘符）。那么在这台电脑上，就不会再显示这些分区了。</p><h3 id="3-5-添加-BIOS-启动"><a href="#3-5-添加-BIOS-启动" class="headerlink" title="3.5 添加 BIOS 启动"></a>3.5 添加 BIOS 启动</h3><p>模仿切换 UEFI 的步骤，切换回 BIOS。</p><p>虚拟机设置 -&gt; 选项 -&gt; 高级 -&gt; 固件类型 -&gt; BIOS</p><p>重新连接 CD/DVD，启动虚拟机。</p><p>现在应该看到这个画面。如果没有看到，请检查是否已经切换回 BIOS。</p><img title="BIOS启动1" data-src="/2020/06/14/将ubuntu装到移动硬盘中即插即用/BIOS启动1.png"><p>选择 Try Ubuntu。</p><p>现在假设您的系统安装在 sdbX（本文为 sdb2）。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo mount /dev/sdbX /mnt</span><br><span class="line">sudo grub-install --target=i386-pc --recheck --boot-directory=/mnt/boot /dev/sdb</span><br></pre></td></tr></tbody></table></figure><p>如果报错：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Installing for i386-pc platform.</span><br><span class="line">grub-install: warning: this GPT partition label contains no BIOS Boot Partition; embedding won't be possible.</span><br><span class="line">grub-install: warning: 无法嵌入。在此次安装中 GRUB 只能通过使用块列表安装。但是块列表是不可信赖的，不推荐使用。.</span><br><span class="line">grub-install：错误： will not proceed with blocklists.</span><br></pre></td></tr></tbody></table></figure><p>请检查是否正确标记了 bios_grub。</p><p>重启一下，看看能不能 BIOS 启动，然后尝试 UEFI 启动。</p><p>如果没有问题，那么，完结撒花！</p><h3 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h3><p>如何设置电脑从U盘启动：</p><p>按住<strong>shift</strong>键点击<strong>重启</strong> -&gt; 选择从USB设备启动</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;将ubuntu装进口袋里&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="linux" scheme="http://www.caoxu.club/categories/linux/"/>
    
      <category term="ubuntu" scheme="http://www.caoxu.club/categories/linux/ubuntu/"/>
    
    
      <category term="Linux" scheme="http://www.caoxu.club/tags/Linux/"/>
    
      <category term="ubuntu" scheme="http://www.caoxu.club/tags/ubuntu/"/>
    
      <category term="操作系统" scheme="http://www.caoxu.club/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>心之力</title>
    <link href="http://www.caoxu.club/2020/05/04/%E5%BF%83%E4%B9%8B%E5%8A%9B/"/>
    <id>http://www.caoxu.club/2020/05/04/心之力/</id>
    <published>2020-05-04T01:07:56.000Z</published>
    <updated>2020-05-04T02:02:02.558Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>宇宙即我心，我心即宇宙。</p><footer><strong>二十八画生</strong><cite>1917年</cite></footer></blockquote><a id="more"></a><p>  宇宙即我心，我心即宇宙。细微至发梢，宏大至天地。世界、宇宙乃至万物皆为思维心力所驱使。博古观今，尤知人类之所以为世间万物之灵长，实为天地间心力最致力于进化者也。夫中华悠悠古国，人文始祖，之所以为万国文明正义道德之始作俑者，实为尘世诸国中最致力于人类与天地万物精神相互养塑者也。盖神州中华，之所以为地球优雅文明之发祥渊源，实为诸人种之最致力于人与社会、天地间公德良知依存共和之道者也。古中华历代先贤道法自然，文武兼备，运筹天下，何等之挥洒自如，何等之英杰伟伦。</p><p>  然天妒九州，外侵内乱，泱泱华夏，愚昧丛生。国人于邪魔强盗阴险心力渗透、攻击治下，神圣使命渐渐失忆，煌煌中华民众却败于众生甘愿自卑、沉沦、散弱之积弊。</p><p>  外侵内斗，有德者心力与物力难济，空有济世情怀。无德者无耻与无畏沆瀣，结为欺世盗贼。丑恶者霸拥民众赋权，神器私用，愚钝者尽情尸位素餐，祸国殃民。贪腐官僚国贼举家富贵，万众民脂民膏皆被劫掠。民心民生衰，则国力日衰，国力衰则国家民族之心力亦衰！内可诱发天灾兵祸，朝代更迭，官僚、商贾、农工、学者皆难免坠落，岂有完卵？外可唆引强盗侵犯，奴役掠夺，国贼、汉奸、军民、学生均家破人亡，罕有善果。迁居蛮夷，神州子民多世代受辱。</p><p>  数十年来，神州大地屡受残暴侵杀，国库民资多被恶吏勾结盗抢殆尽。甲午海战、八国联军、沙俄侵略、21条…不平等诸般卖国条约卑下怯弱，引狼入室，资敌来犯，万民屈辱，愚政破败，实为召唤、鼓励蛮夷强盗分食华夏之举。</p><p>  与蛮夷通商者使洋货泛滥，居高居奇，国人尽被盘剥。泱泱中华竟无力生产民众生活诸品，更无官僚执权为民众艰辛解忧谋福。世界诸国强盗在中国如入空境，颇有斩获，无不惊叹欢呼中国之可欺。故恶敌觊觎长存，亡我之心不死！只有寻机死战拒敌，方可换得暂时太平。炎炎烈战未开，巍巍国耻未雪，则蛮夷、豺狼、凶魔如食甘饴，纷沓而至。</p><p>  洋奴横行，汉奸猖獗，国民皆因腐败汉奸官僚、军阀、买办家族所欠洋人之无尽亡国债务而百世自危。国体破败，军阀割据，混战连年，国债深陷，物价飞涨，食宿艰难，民俗劣化，灾厄连连，何日可止？今满清鞑虏虽败，可恨国家、政治、经济均被愚昧独夫、洋奴把持，国民心力沉疴虚弱，蛮夷恶敌肆意摧残，恍惚间，惊见万民为奴，国资殆尽。</p><p>  若欲救民治国，兴中华英武，虽百废待兴，可铸奇造伟，成我辈绝伦。救国救民计，惟有自强国民心力之道乃首要纲领，然民众思维心力变新、强悍者是为首要之捷径！</p><p>  心力变新、强悍者首应破除封建、官僚之愚昧邪道，惩治卖国、汉奸、洋买办之洋奴愚众，明戒其不义浮财罚入公帑，暗布其家族子女皆无善终，方可尽教化之道。</p><p>  举世兴原创睿智，立国显始做宏略。国家民族之新生心力志向，必缔造世界仁德勇武文明之新学，新学为思想理论之基石、栋梁，新学不兴，御敌难成。</p><p>  西方学教均显邪佞，如若任其纵横世间，则人文尽毁。如神州中华新学宏论集古今大成之时，必为人类之新邦。</p><p>  中华古国之敌皆为西方邪恶之魔盗与汉奸，与倭寇同仇，此仇无解，解则自毁，切忌隐忍淡忘，万勿愚善助恶。</p><p>  今力主洋务之事应借鉴“师夷之长以制夷”之道，尽知“非我族类其心必异，非我族者其性必恶”之祖训。与洋盗交往，睚眦必报，以雷霆手段显菩萨心肠，击杀敌酋，方可博取中华国邦民众之自强尊严。<code>细观西方魔盗侵杀之秘技，往往以掠夺之技巧财富造就全球侵杀之辎重，以伪善之攻心宗教幻化万国奴役之中枢。</code>故无数肖小愚昧之中国政客尽被麻痹蛊惑迷信，以自卑万漏国体媚洋为奴，贱卖民脂国魂。</p><p>  留洋之风，不可放纵，国学之巅，大有裨益。故救国者必在国内，留洋者志不可晦。留洋列强之同胞漂泊海外，须警惕邪魔强盗对我正义灵魂之误导、侵扰。有如寒冬之防风，病弱之辟邪。攻防有度，张弛有道，则洋魔、汉奸难以得手，于海内外皆无处安身，惩前毖后，可大绝之。中华栋梁必生自主自强之新象，设抗御蛮夷强盗杀戮之预防策，扬神州民众富国雄军之壮志，恤民生农工商学之福祉滋养。开创新学润养新民，辅以新民楷模国民。抗击蛮夷列强剿灭中华神圣传承之奸计，履行万国大同正义道德教化优靖之使命。</p><p>  夫闻“<strong>三军可夺其帅，匹夫不可夺其志</strong>”。志者，心力者也。民之志首推举国民众个性之天然强健，则国家栋梁层出不穷。数百年外侮内斗中民众个性屡被君主官僚残害之积弊甚重，国民心性身体多有贻害，民之弱即国之弱。举国凡有压抑个人、违背国民个性者，罪莫大焉！故我国三纲所在必去，愚民愚治尽除，方有优塑民众强盛之希冀。</p><p>  自中国开埠以来，封建、官僚、愚民、洋务祸国殃民，究其缘由，乃教会、资本家、君主、卖国贼四者。四贼结伴，犹如鬼魅之食人联盟，皆为天下恶魔强盗者也。</p><p>  四贼之中，尤以执掌政权之官僚当为祸首！盖国之神圣重器以民为先，决不可助长恶私贪欲窃为己用！国之中枢如有愚昧肤浅肖小之徒窃居，则外魔必侵杀，国民必衰亡。</p><p>  自满清鞑虏洋务运动之后，贪墨腐败家族皆以盗取、盘剥民脂民膏逃逸海外为家族享乐之诡计。假以时日，神州中华亿万民众祖辈之血肉、骨髓乃至福祉，将被尽数剥夺转送西方魔盗与贪腐家族！国人如寒冬之时又堕深渊，垂死之躯更遭荼毒。农业之国民众落后狭隘，必将沦落为亡国奴邦。劫掠国民财富者逃逸海外，至今无人于海内查处、治罪，于海外统计、堵截，故国贼趋之若鹜。吾辈倘若不能海内惩戒、海外肃杀，又与国贼、禽兽何异？又岂能根除国贼汉奸？</p><p>  <code>千古圣人，教化为根。我辈恰逢此乱象当前之世，人皆逐物欲而迷心，循末节而忘真，醉娱乐轻国志，谋小私绝大利，认蛮夷做乃父，拜魔盗为师尊，毁文明于无耻。你我何必苟且偷生，熟视无睹？有志者呼吸难畅，应以天下为己任，救黎民于水火，济众生远奸佞。</code></p><p>  今愚者忘本堕渊，竟争先自掘其坟，却不思国家民族社稷危亡之计。苟活于当下，遗失神圣之使命，忘却民族之重任；背离于真理，违逆人本之慧根，蔑毁先民之道德；醉心于享乐，不知当世之惊变，甘当媚外之洋奴；沉迷于自我，罔顾危机之四伏，轻信魔盗之谗言！故西方一众强盗皆可肆意侵杀、掠夺、奴役我汉唐中华，千百年来血债累累。</p><p>  当今世界之格局风云激荡，人类文明之前途扑朔渺茫，天下苍生之幸福岌岌可危。<code>虽有科技带来物质之充足，仍难满人欲之巨壑，各派皆为私利而竞相奔走，人人皆溺恶欲之堕落洪流。强盗流氓制订裁决世界野蛮法律，邪恶魔鬼公然成为人间伪善领袖，万国不思兴道义之师，竟全然拜魔盗为导师，此星球之一草一木万物生灵涂炭、灭绝之期不远矣。</code></p><p>  <code>虽有智者、勇者愿做中流之砥柱，却犹如闹市之人语，瀑下之鱼鸣。</code>请问周边，还有几人执著于真理？还有几人探求于本源？<code>一句开心就好，便甘愿随波逐流；一句事不关己，便通行四海愚夫；一句莫谈国事，便据民权为私器。孰不知天下兴亡匹夫有责？试问为天地立心何以立？为生民立命何以立？为往圣继绝学何以继？为万世开太平何以开？</code>若我辈之人此心已无，则中华即将亡亦！中华亡则人类必亡亦！</p><p>  <code>天之力莫大于日，地之力莫大于电，人之力莫大于心。</code>阳气发处，金石亦透，精神一到，何事不成？改朝换代，为民谋福，惩治贪墨汉奸，又有何难！苟其公忠体国，百折不回，<code>虽布衣下士，未始无转移世运之能也。有志之士可不勉哉！</code>人生于天地之间，形而下者曰血肉之躯，形而上者曰真心实性。血肉者化物质之所成，心性者先天地之所生。故而有唯物唯心之论说。人活于世间，血肉乃器具，心性为主使，神志为天道。血肉现生灭之相，心性存不变之质，一切有灵生命皆与此理不悖。盖古今所有文明之真相，皆发于心性而成于物质。德政、文学、艺术、器物<code>乃至个人所作所为均为愿、欲、情等驱使所生。</code></p><p>  故个人有何心性即外表为其生活，团体有何心性即外表为其事业，国家有何心性即外表为其文明，众生有何心性即外表为其业力果报。故心为形成世间器物之原力，<code>佛曰：心生种种法生，心灭种种法灭。</code>西方强盗宗教亦有旧约主神虐民之邪暴，后有耶稣新约爱民之佛性改良。神魔心性之变幻如此，故世人多为耶稣所迷。耶稣明之故说忏悔，懂耻而不恶；孔子明之故说修心，知止而不怠；释迦明之故说三乘，明心而不愚；老子明之故说无为，清静而不私。<code>心为万力之本，由内向外则可生善、可生恶、可创造、可破坏。由外向内则可染污、可牵引、可顺受、可违逆。</code>修之以正则可造化众生，修之以邪则能涂炭生灵。心之伟力如斯，国士者不可不察。大丈夫立天地间，借浩然正气养明德之志。</p><p>  <code>大凡英雄豪杰之行其自己也，确立伟志，发其动力，奋发踔历，摧陷廓清，一往无前。</code>其强大如大风之发于长合，如好色者朱之性欲发动而寻其情人，决无有能阻回之者，亦决不可有阻者。尚阻回之，则势力消失矣。吾尝观大来勇将之在战阵，有万夫莫当之概，发横之人，其力至猛，皆由其一无顾忌，其动力为直线之进行，无阻回无消失，所以至刚而至强也。众生心性本同，豪杰与圣贤之精神亦然。</p><p>  <code>故当世青年之责任，在承前启后继古圣百家之所长，开放胸怀融东西文明之精粹，新研奇技巧器胜列强之产业，与时俱进应当世时局之变幻，解放思想创一代精神之伟烈。破教派之桎楛，汇科学之精华，树强国之楷模，布真理于天下！今正本清源，愿与志同道合、追求济世、救世真理者携手共进，发此弘愿，世世不辍，贡献身心，护持正义道德。</code></p><p>  <code>今吾辈任重而道远，若能立此大心，聚爱成行，则此荧荧之光必点通天之亮，星星之火必成燎原之势，翻天覆地，扭转乾坤。</code>戒海内贪腐之国贼，惩海外汉奸之子嗣；养万民经济之财富，兴大国农工之格局；开仁武世界之先河，灭魔盗国际之基石；创中华新纪之强国，造国民千秋之福祉；兴神州万代之盛世，开全球永久之太平！也未为不可。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;&lt;p&gt;宇宙即我心，我心即宇宙。&lt;/p&gt;
&lt;footer&gt;&lt;strong&gt;二十八画生&lt;/strong&gt;&lt;cite&gt;1917年&lt;/cite&gt;&lt;/footer&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="转载" scheme="http://www.caoxu.club/categories/%E8%BD%AC%E8%BD%BD/"/>
    
    
      <category term="名人文章" scheme="http://www.caoxu.club/tags/%E5%90%8D%E4%BA%BA%E6%96%87%E7%AB%A0/"/>
    
  </entry>
  
  <entry>
    <title>基于Wi-Fi数据的公交 Trip-level OD矩阵估计</title>
    <link href="http://www.caoxu.club/2020/02/12/WiFi-detect-OD-estimation/"/>
    <id>http://www.caoxu.club/2020/02/12/WiFi-detect-OD-estimation/</id>
    <published>2020-02-12T05:03:51.000Z</published>
    <updated>2020-02-13T08:43:48.073Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><em>武汉加油！中国加油！</em></p></blockquote><p>2020年第一篇博客，首先来填坑。本项目搭建了Wi-Fi检测信息分析网站，介绍了基于Wi-Fi信号检测的公交OD估计方法。这是本科两个毕设（软件工程+交通工程）的结合。<br><a id="more"></a></p><div class="note default">            <h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2>          </div><p>Origin-destination (OD) data of bus passenger flow is useful for improving the bus service. The OD matrix contains information about where passenger board and alight the bus. It can be used to design a new bus route, new stations, change the route structure, improve vehicle scheduling and driver scheduling (<em>1</em>). Currently, passenger demand information can be obtained through Automatic Passenger Counter (APC) systems (<em>2</em>), Automatic Fare Collection (AFC) systems (<em>3</em>). But those data may only be used to find the origin of passengers and not their destinations, and ride time. According to the <span class="exturl" data-url="aHR0cHM6Ly93d3cuY2lzY28uY29tL2MvZW4vdXMvc29sdXRpb25zL2NvbGxhdGVyYWwvc2VydmljZS1wcm92aWRlci92aXN1YWwtbmV0d29ya2luZy1pbmRleC12bmkvd2hpdGUtcGFwZXItYzExLTc0MTQ5MC5odG1s" title="https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/white-paper-c11-741490.html">Cisco Visual Networking Index: Forecast and Trends, 2017-2022 White Paper<i class="fa fa-external-link"></i></span> predicts that in 2020, more than 53% of IP traffic in the global market will come from Wi-Fi devices. In addition, the report predicts that the number of Wi-Fi access points worldwide will increase year by year, from 64.2 million in 2015 to 432.5 million in 2020 (<em>4</em>). With the popularity of smart mobile communication devices, passengers will carry mobile devices with them during the trip. Mobile devices such as smartphones, tablets, and laptops constantly emit Wi-Fi signals. These signals can be distinguished by the device’s unique Media Access Control (MAC) address. Note that every MAC address is unique to its device. It is possible to obtain an OD matrix by detecting these device’s MAC addresses.</p><p>Recently, a large number of studies have been conducted to obtain Bus passenger flow data based on Wi-Fi signals. For collecting Wi-Fi signals data of mobile devices, a Wi-Fi detection system needs to be built, Mikkelsen et al. (<em>5</em>) proposed a rapid prototype of the detection system using a Raspberry Pi and a Wi-Fi network card. As the detected Wi-Fi signals may come from devices outside the bus, classifying them as originating from passenger or non-passenger devices is necessary (<em>5;6;7</em>). Oransirikul et al. (<em>7</em>) proposes a real-time filtering mechanism with received signal strength indication (RSSI) as the filtering parameter, with an accuracy rate of 75%. Some studies use detection data to estimate passenger board and alight stop, and then generate an OD matrix for Wi-Fi devices (<em>3;8</em>). In addition, the algorithms that estimated the actual OD flow matrix based on the Wi-Fi device OD matrix have been proposed (<em>8;9</em>). Ji et al. (<em>8</em>) propose a hierarchical Bayesian model to estimate trip-level OD flow matrices using sampled OD flow data and boarding data provided by fareboxes.</p><p>The objective of this paper is to estimate the trip-level OD matrix by detecting Wi-Fi data in the bus combined with passenger boarding and alighting counts. The contributions of this work are as follows:</p><p>(1) Design a Maximum Likelihood Estimator (MLE) method to estimate the actual trip-level OD matrices. </p><p>(2) Propose an arrival time matching method to infer the OD matrices of Wi-Fi devices.</p><p>(3) Developed <span class="exturl" data-url="aHR0cDovL3d3dy5kanR1YnVzdG9vbC5jb20=" title="http://www.djtubustool.com">djtubustool<i class="fa fa-external-link"></i></span> website for Wi-Fi signals data collection, storage, and computation.</p><p>The remainder of this paper is organized as follows: Section 2 presents the data collection system, including Wi-Fi sensors, detection data analysis website. Section 3 introduces Methodologies, including detection data filtering methods, Wi-Fi device OD matrices and actual OD matrices estimation methods, and performance metrics. Section 4 introduces the process of empirical evaluation and analyzes its results. Section 5 presents the conclusions and proposes future work.</p><div class="note default">            <h2 id="2-Data-collection-system"><a href="#2-Data-collection-system" class="headerlink" title="2. Data collection system"></a>2. Data collection system</h2>          </div><h3 id="2-1-System-design"><a href="#2-1-System-design" class="headerlink" title="2.1 System design"></a>2.1 System design</h3><p>The data collection system is divided into two parts, including the hardware system and the data analysis website. The hardware system is responsible for collecting and uploading Wi-Fi data on the bus. After the Wi-Fi sensor completes data collection, this paper develops a <span class="exturl" data-url="aHR0cDovL3d3dy5kanR1YnVzdG9vbC5jb20=" title="http://www.djtubustool.com">data analysis website<i class="fa fa-external-link"></i></span>. This website provides efficient data storage, calculation, and display, as well as passenger boarding and alighting counts of manual surveys. The architecture of the system is shown in Figure 1.</p><img title="Overall architecture of the data collection system" data-src="/2020/02/12/WiFi-detect-OD-estimation/Overall%20architecture%20of%20the%20data%20collection%20system.png"><center>Figure 1  Overall architecture of the data collection system</center><p>When the detect time reaches a fixed period, the Wi-Fi sensor will switch from the monitor mode to the connect mode and actively establish a connection with the previously set network. Wi-Fi sensors upload data and then switch to monitor mode after the transmission is complete.</p><p>The data receiving module on the server side receives the data packet sent by the detection module, and stores it in the database after parsing. Investigators can access the website at any terminal to monitor the Wi-Fi detection data in real-time. After the investigation experiment is completed, the investigators can perform data calculations on the web page, and calculate the OD matrices of the bus route based on the manual survey results and Wi-Fi detection data.</p><h3 id="2-2-Hardware-system"><a href="#2-2-Hardware-system" class="headerlink" title="2.2 Hardware system"></a>2.2 Hardware system</h3><p>In the IEEE 802.11 communication protocol, a device needs to be “discovered” by the network before it is connected to a Wi-Fi access point. Probe request management frames are designed to solve the “discovery” problem of access points. When mobile devices are not connected to the access point, mobile devices with Wi-Fi enabled will periodically send probe request frames to actively discover nearby access points. Within the detection range of the Wi-Fi sensor, the Wi-Fi sensor can receive a probe request from a Wi-Fi device in monitor mode and extracts the device’s MAC address. By continuously detecting the probe request frames of the mobile device in the bus, and uploading the collected detection data to the database of the server, real-time detection of the mobile device can be achieved.</p><p>The hardware of the data collection system consists of three parts, including Wi-Fi sensors, mobile power bank, and mobile network connections. Figure 2 shows the composition of the hardware of the data collection system.</p><img title="The composition of the hardware of the data collection system" data-src="/2020/02/12/WiFi-detect-OD-estimation/The%20composition%20of%20the%20hardware%20of%20the%20data%20collection%20system.png"><center>Figure 2  The composition of the hardware of the data collection system</center><p>The Wi-Fi sensor uploads the detected Wi-Fi signal data at a fixed period, and the detection range is about 50 meters. Wi-Fi sensors can collect the following data: source MAC address, destination MAC address and signal strength (RSSI). Source MAC address is the MAC address of the detected device. Destination MAC address is the MAC address of the device to which the packet was sent. Mobile network access points provide network connectivity for data uploads. Power bank provides power for Wi-Fi sensor’s run.</p><div class="note default">            <h2 id="3-Methodologies"><a href="#3-Methodologies" class="headerlink" title="3. Methodologies"></a>3. Methodologies</h2>          </div><h3 id="3-1-Data-filtering"><a href="#3-1-Data-filtering" class="headerlink" title="3.1 Data filtering"></a>3.1 Data filtering</h3><p>Since the detected signal data may come from pedestrians, vehicles, and buildings outside the bus, it is necessary to filter the data of the devices outside the bus. Some studies have tried using data pre-processing steps to reduce noise data (<em>5</em>). In this study, from the perspective of the duration of the device detection, it can be considered that the device with a short detection duration may be a device that was briefly detected outside the bus. Therefore, the detection duration is selected as a parameter for filtering signal data. Let T represent the duration of the device detection and <script type="math/tex">T_{\min }</script> represents the minimum duration. MAC address data with device detection duration less than $T_{\min }$ will be removed:</p><center>$$T&gt;T_{\min }$$</center><p>From the perspective of the distance between the device and the sensor, it considers that devices with long distances do not exist in the bus, so the average signal strength is selected as a parameter to filter the signal data. Use <script type="math/tex">S_{\text {average}}</script> to represent the average signal strength of the device, <script type="math/tex">S_{\min}</script> to represent the minimum average signal strength, and the detection device data with the average signal strength less than <script type="math/tex">S_{\min}</script> will be filtered:</p><center>$$S_{\text {average}}&gt;S_{\min }$$</center><h3 id="3-2-Wi-Fi-device-OD-inference-based-on-time-matching"><a href="#3-2-Wi-Fi-device-OD-inference-based-on-time-matching" class="headerlink" title="3.2 Wi-Fi device OD inference based on time matching"></a>3.2 Wi-Fi device OD inference based on time matching</h3><p>For the inference of the boarding stop of the device, find the stop where the arrival stop time closest to the device’s first detect timestamp, and earlier than the first detect timestamp. For the inference of the alighting stop of the device, find the stop where the arrival stop time closest to the device’s last detect timestamp, and later than the last detect timestamp. The inference process of the boarding and alighting stop of the detection devices is shown in Figure 3.</p><img title="The inference process of the boarding and alighting stop of the detected devices" data-src="/2020/02/12/WiFi-detect-OD-estimation/The%20inference%20process%20of%20the%20boarding%20and%20alighting%20stop%20of%20the%20detected%20devices.png"><center>Figure 3   The inference process of the boarding and alighting stop of the detected devices</center><p>The OD matrix estimation steps of Wi-Fi devices are as follows:</p><p><strong>Step1</strong>: Infer the boarding stop of the device. Compare the arrival time of each stop in sequence with the time when the device was first detected, and find the first stop whose arrival time is greater than the first detection time. The previous stop at this stop is considered to be the boarding stop of the device. boarding stop is given by:</p><center>$$\left.\begin{array}{rl}{t_{m}-t_{\text {firstbetect}}} &amp; {=\min \left(t_{x}-t_{\text {firstDetect}}\right)} \\{\text {s.t.}\left\{\begin{array}{l}{t_{x}-t_{\text {firstDetect}}&gt;0} \\{x \in(1, \cdots, k)}\end{array}\right.}\end{array}\right\} \Rightarrow S_{\text {boarding}}=S_{m-1}$$</center><p>Where <script type="math/tex">S_{boarding}</script> represents the boarding stop, <script type="math/tex">t_{firstDetect}</script> represents the first detection time, <script type="math/tex">t_{x}</script> represents the arrival time of the x stop, and k represents the stop number.</p><p><strong>Step2</strong>: Infer the alighting stop of the device. Compare the arrival time of each stop in sequence with the last time the device was detected, and find the first stop whose arrival time is greater than the last detection time. Consider this stop as the alighting stop for the device. the alighting stop is given by:</p><center>$$\left.\begin{array}{rl}{t_{n}-t_{\text {lastDetect}}} &amp; {=\min \left(t_{x}-t_{\text {lastDetect}}\right)} \\{s . t .\left\{\begin{array}{l}{t_{x}-t_{\text {lastDetect}}&gt;0} \\{x \in(1, \cdots, k)}\end{array}\right.}\end{array}\right\} \Rightarrow S_{\text {alighting}}=S_{n}$$</center><p>Where <script type="math/tex">S_{alighting}</script> represents the alighting stop, and <script type="math/tex">t_{lastDetect}</script> represents the last detection time.</p><p><strong>Step3</strong>: Accumulate the number of devices in Wi-Fi device OD matrix according to the inferred corresponding positions of the boarding and alighting stop:</p><center>$$Q\left(S_{\text {boarding}}, S_{\text {aligiting}}\right)=Q\left(S_{\text {boarding}}, S_{\text {aligiting}}\right)+1$$</center><p>where <script type="math/tex">Q(S_{boarding},S_{alighting})</script> represents the number of devices which boarded at <script type="math/tex">S_{boarding}</script> and got off at <script type="math/tex">S_{alighting}</script>.</p><p>Repeat steps 1 to 3 until the data inference for all devices is complete. The Wi-Fi device OD matrix based on time matching can be obtained as shown in Table 1:</p><center>Table 1  The Wi-Fi device OD matrix based on time matching</center><img title="The Wi-Fi device OD matrix based on time matching" data-src="/2020/02/12/WiFi-detect-OD-estimation/The%20Wi-Fi%20device%20OD%20matrix%20based%20on%20time%20matching.jpg"><p>where <script type="math/tex">B(i)</script> represents the number of detection devices originating from stop i, and <script type="math/tex">A(j)</script> represents the number of detection devices alighting at stop j.</p><p>And the bus load L(x) is given by:</p><center>$$\begin{aligned}L(x)=&amp; L(x-1)+B(x)-A(x) \\&amp; x \in(2, \ldots, k-1)\end{aligned}$$</center><p>where L(x) represents the bus load from stop x to stop x+1, and L(1)=B(1).</p><h3 id="3-3-Actual-OD-estimation"><a href="#3-3-Actual-OD-estimation" class="headerlink" title="3.3 Actual OD estimation"></a>3.3 Actual OD estimation</h3><p>In order to estimate the actual OD matrix using the Wi-Fi detection OD matrix, according to the method proposed by Ben-Akiva (<em>10</em>), a Maximum Likelihood Estimator (MLE) method is proposed in this paper. The following are assumptions for maximum likelihood estimation:</p><p>(1) Passengers arrive individually and according to a Poisson process.</p><p>(2) There is negligible bias in Wi-Fi OD matrices data.</p><p>(3) The boarding and alighting counts are independent of each other.</p><p>Assumptions (1) to (3) are considered reasonable in most cases (<em>11</em>). In this study, we usually use a Wi-Fi OD matrix and the boarding and alighting counts for MLE.</p><p>As the MLE, the mean value of this Poisson process is the detected arrival rate for OD pair i,j: <script type="math/tex">\lambda_{i j} p_{i j}</script></p><center>$$P\left(Q(i, j) | \lambda_{i j}, p_{i j}\right)=e^{-\lambda_{i j} p_{i j}} \frac{\left(\lambda_{i j} p_{i j}\right)^{Q(i, j)}}{Q(i, j) !}$$</center><p>Where <script type="math/tex">lambda_{ij}</script> is the mean value of the Poisson process for passenger arrivals with origin i and destination j, and <script type="math/tex">p_{ij}</script> is the percentage of passenger flows from i to j detected by the Wi-Fi sensor.</p><p>The total probability or likelihood of observing all the data is:</p><center>$$L(Q(i, j))=\prod_{i} \prod_{j} e^{-\lambda_{i j} p_{i j}} \frac{\left(\lambda_{i j} p_{i j}\right)^{Q(i, j)}}{Q(i, j) !}$$</center><p>Taking the natural log on both sides:</p><center>$$\ln L(Q(i, j))=\sum_{i} \sum_{i}\left(-\lambda_{i j} p_{i j}+Q(i, j) \ln \lambda_{i j} p_{i j}\right)+C$$</center><p>Where C is a constant. Since <script type="math/tex">p_{ij}</script> is also unknown, it is impossible to estimate <script type="math/tex">lambda_{ij}</script> and <script type="math/tex">p_{ij}</script> at the same time. By assuming that the detect rate <script type="math/tex">p_{ij}</script> is dependent on the origin and destination stop, as the study (<em>10</em>) proposes, we can replace <script type="math/tex">p_{ij}</script> by <script type="math/tex">p_{i} * p_{j}</script>:</p><center>$$\ln L(Q(i, j))=\sum_{i} \sum_{j}\left(-\lambda_{i j} p_{i} p_{j}+Q(i, j) \ln \lambda_{i j} p_{i} p_{j}\right)+c$$</center><p>To solve for <script type="math/tex">\max _{\lambda_{i j}} \ln L(Q(i, j))</script>, we can take derivatives on both sides over <script type="math/tex">lambda_{ij}</script>, and set the derivatives to zero, and solve the resulting equation:</p><center>$$\frac{\partial \ln L(Q(i, j))}{\partial \lambda_{i j}}=-p_{i} p_{j}+\frac{Q(i, j)}{\lambda_{i j}}=0, \forall i, j$$</center><p>Therefore, the maximum likelihood estimates for <script type="math/tex">lambda_{ij}</script>is:</p><center>$$\widetilde{\lambda_{i j}}=\frac{Q(i, j)}{p_{i} p_{j}}, \forall i, j$$</center><p>Where <script type="math/tex">\widetilde{\lambda_{i j}}</script> is the value of maximum likelihood estimates for each OD pair.</p><div class="note default">            <h2 id="4-Empirical-evaluation"><a href="#4-Empirical-evaluation" class="headerlink" title="4. Empirical evaluation"></a>4. Empirical evaluation</h2>          </div><h3 id="4-1-Experiment-design"><a href="#4-1-Experiment-design" class="headerlink" title="4.1 Experiment design"></a>4.1 Experiment design</h3><p>As shown in Figure 4, this paper selected the Dalian 101 bus route for evaluation. The bus route runs from Malan Square in the west to Dalian Railway Station in the east and has 11 stops. The direction of the route investigated in this study is from Malan Square to Dalian Railway Station.</p><img title="Survey route route 101" data-src="/2020/02/12/WiFi-detect-OD-estimation/Survey%20route%20route%20101.png"><center>Figure 4  Survey route: route 101</center><p>The survey was conducted from April 8 to April 13, 2019, including weekdays and weekends. The survey period includes four periods of 7: 00-8: 00, 8: 00-9: 00, 16: 00-17: 00, and 17: 00-18: 00. The survey data includes Wi-Fi signal data and boarding and alighting counts at each stop. The on-board survey includes the number of passengers boarding and alighting the bus at each stop, the arrival time and the departure time of each stop. A total of 10,517 Wi-Fi signals were detected from 686 mobile devices.</p><h3 id="4-2-Data-cleaning"><a href="#4-2-Data-cleaning" class="headerlink" title="4.2  Data cleaning"></a>4.2  Data cleaning</h3><p>The detection range of the Wi-Fi sensor is about 50 meters. When the sensor is installed in the bus, the Wi-Fi signal outside the bus is inevitably detected. In this study, the minimum duration and minimum average signal strength were designed as filtering parameters. The procedure for determining parameter values is as follows:</p><p><em>Step1</em>: Determine a set of parameters according to 10%(40s), 20%(80s), 30%(120s) of the cumulative frequency of the device detection duration and 10%(-94dBm), 20%(-93dBm), 30%(-92dBm) of the cumulative frequency of the average signal strength respectively, and combine them to get 9 filtering parameter pairs as the table 2 shows, use these parameters to filter the data.</p><p><em>Step2</em>: Find the minimum value of the variance of the ratio of the detected Bus load to the actual bus load, and its corresponding filtering parameter value is considered have the most stable filtering effect, which is the optimal filtering parameter value.</p><p><em>Step3</em>: Count the optimal filtering parameter values of each trip dataset, and determine the value of the filtering parameter as the parameter pair that has become the optimal filtering parameter most times.<br>According to the above steps, 9 sets of candidate parameters are determined to filter the 10 trip datasets, and the number of times that these 9 parameter pairs become the optimal filtering parameters is counted, as present in Table 2.</p><center>Table 2  Statistics of optimal filtering parameters</center><img title="Statistics of optimal filtering parameters" data-src="/2020/02/12/WiFi-detect-OD-estimation/Statistics%20of%20optimal%20filtering%20parameters.png"><p>From the statistical results in the table above, the minimum detection duration and minimum average signal strength were determined to be 120 seconds and -92 dBm, respectively.</p><h3 id="4-3-Performance-metrics"><a href="#4-3-Performance-metrics" class="headerlink" title="4.3 Performance metrics"></a>4.3 Performance metrics</h3><p>The objective of this paper is to obtain a good OD matrix estimation based on the OD matrix of Wi-Fi devices. Due to the labor-intensive and time-consuming investigation of the actual OD matrix, we cannot directly obtain ground truth OD data for measurement. The bus load is calculated by summing each row and column of the estimated OD, and the accuracy of the estimated OD can be evaluated by comparing the estimated bus load with the actual bus load. The evaluation metrics of bus load uses the average bus load difference G, G is given by:</p><center>$$G=\frac{1}{K} \sum_{k} \frac{\sum_{n}\left|\widetilde{B L_{k}(n)}-B L_{k}(n)\right|}{N}$$</center><p>where <script type="math/tex">\widetilde{BL}_{k}(n)</script> represents the estimated bus load between stop n and stop n+1 in trip k, and <script type="math/tex">{B L}_{k}(n)</script> represents the actual bus loads between stop n and stop n+1. The higher G is, the lower the estimation accuracy is.</p><h3 id="4-4-Estimate-result"><a href="#4-4-Estimate-result" class="headerlink" title="4.4 Estimate result"></a>4.4 Estimate result</h3><p>In this part, we compare the performance of the MLE and a proportional fitting (PF) method in terms of bus load of each trip. For comparison, a simple, easy-to-implement proportional fitting (PF) method is also used for evaluation. For each cell of the base matrix multiplied by the proportion of actual to detected boarding counts, the actual OD flow matrix E(i,j) is given by:</p><center>$$E(i, j)=B_{t}(i) \times \frac{Q(i, j)}{\sum_{k} Q(i, k)}$$</center><p>where <script type="math/tex">B_{t}(i)</script> represents the count of passengers boarding at the stop i.</p><p>“Structural zero” problem (<em>10</em>) occurred when using the PF method to calculate. The problem is that at a given stop, the number of detected devices boarding the bus is zero, but the number of passengers boarding the bus is not zero, which results in the updated value of OD pairs boarding from this stop still be zero. In this paper, solve this problem by changing the “0” cells of the base matrix to “1”.</p><p>Figure 5 shows the estimated and true bus load profiles for each bus trip. Trip1, 3, 5, 7, 9, and 11 are the survey results in the morning (7: 00-8: 00), while trip2, 4, 6, 8, 10, and 12 are the afternoon (16: 00-17: 00) results. For the former, the maximum segments of the bus load are between stop 5 and stop 7, and for the latter, there are two maximum segments that stop 4 to stop 6(for trip 2,6,10) and stop 6 to stop 8(for trip 4,8,12). Obviously, the variation of passenger demand for different periods leads to large trip-level demand variation. In addition, variations in land use near the stop also lead to variations in passenger demand. For example, the sixth stop is Xi’an road stop. It is near a large commercial center with a large number of commuters, so the bus load before the sixth stop in the morning peak is high. Trip 1 to 8 are weekday survey data and trips 9 to 12 are weekend survey data. The survey data has different passengers’ demands on different dates, also leading to the variation of bus load.</p><img title="Estimated and ground truth of bus load" data-src="/2020/02/12/WiFi-detect-OD-estimation/Estimated%20and%20ground%20truth%20of%20bus%20load.png"><center>Figure 5  Estimated and ground truth of bus load</center><p>As shown in figure 5, the bus load estimated by the MLE method shows a relatively higher degree of similarity with the corresponding true ones. Therefore, the MLE method can be used to estimate the OD flow with a certain accuracy. As a comparison, the simple PF method may have poor bus load estimates on some bus trips, such as trips 5 and 9.</p><p>In order to quantify the estimation error, we statistics the distribution of the estimated bus load error. Figure 6 presents the fitting probability density functions (PDFs) of the estimated error for the MLE and the PF method. Positive error represents the load is overestimated.</p><img title="Estimated bus load error distribution" data-src="/2020/02/12/WiFi-detect-OD-estimation/Estimated%20bus%20load%20error%20distribution.png"><center>Figure 6  Estimated bus load error distribution</center><p>Figure 6 reveals that the MLE method provides better bus load estimates than the PF method. In the figure 6, estimates errors of the MLE method are distributed around “0” mostly, while estimates errors are concentrated around “3” for the PF method. In general, the average absolute error value G is equal to 3.83 for the MLE and 4.96 for the PF. The PF method tends to overestimate the bus load. The PF method has estimation errors that might due to the proportion of passengers boarding at a given stop carrying a Wi-Fi device is not constant.</p><p>Figure 7 shows the cumulative density functions (CDFs) of the error for the two algorithms base on 12 trips. The figure indicates the MLE method outperforms the PF method. For instance, for about 80% of the bus loads, the MLE method leads to an error lower than 6. For the PF method, the error is lower than 6 only about 67% of the loads. Therefore, MLE can provide a more accurate estimation of the bus load.</p><img title="CDF of the bus load error" data-src="/2020/02/12/WiFi-detect-OD-estimation/CDF%20of%20the%20bus%20load%20error.png"><center>Figure 7  CDF of the bus load error</center><div class="note default">            <h2 id="5-Conclusion-and-future-work"><a href="#5-Conclusion-and-future-work" class="headerlink" title="5. Conclusion and future work"></a>5. Conclusion and future work</h2>          </div><h3 id="5-1-Conclusion"><a href="#5-1-Conclusion" class="headerlink" title="5.1 Conclusion"></a>5.1 Conclusion</h3><p>The bus is an important part of the urban public transportation system, and the OD flow data of bus passengers is the basic data for bus operation management and planning. This paper studies the estimation of bus OD matrices based on Wi-Fi signal data. The data collection system built in this paper is divided into two parts, including the hardware system and the data analysis website. The process of collecting, storing, and calculating Wi-Fi data and manual survey data is integrated into the website(www.djtubustool.com), improving the efficiency of conducting survey experiments. And the website is available for anyone who wants to repeat the work in this paper. A method estimating the OD matrices of Wi-Fi devices based on time matching and a Maximum Likelihood Estimator (MLE) method for the actual OD flow matrices estimation are proposed. Data filtering methods based on minimum detection duration threshold and minimum average signal strength threshold are also proposed. According to the empirical evaluation, the MLE method can provide good trip-level OD matrices estimation with the average bus load error is 3.83.</p><h3 id="5-2-Future-work"><a href="#5-2-Future-work" class="headerlink" title="5.2 Future work"></a>5.2 Future work</h3><p>Future research needs to determine the uncertainty of the ratio of the detected OD flow to the actual OD flow to improve the accuracy of the estimation. MAC address randomization technology enables a device to transmit multiple MAC addresses. Future research needs to address the identification challenges brought by MAC address randomization.</p><div class="note success">            <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>  这个篇文章是对大学本科大创、毕设的总结，在2020年这个不平凡的寒假画上了句号，未来还有更多挑战等待着去完成，加油！</p>          </div><div class="note default">            <h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2>          </div><ol><li>Ceder, A. (2016). Public transit planning and operation: Modeling, practice and behavior.</li><li>Ji, Y., Mishalani, R. G., &amp; McCord, M. R. (2015). Transit passenger origin–destination flow estimation: Efficiently combining onboard survey and large automatic passenger count datasets. Transportation Research Part C: Emerging Technologies, 58, 178-192.</li><li>Mishalani, R. G., McCord, M. R., &amp; Reinhold, T. (2016). Use of Mobile Device Wireless Signals to Determine Transit Route-Level Passenger Origin–Destination Flows: Methodology and Empirical Evaluation. Transportation Research Record, 2544(1), 123-130.</li><li>Cisco, V. N. I. (2019). Cisco Visual Networking Index: Forecast and Trends, 2017–2022 White Paper. Porto Salvo, Lisboa. Disponível em:&lt; www. cisco. com/c/pt_pt/about/press/news-archive-2018/20181127. html&gt;, Acesso em, 17.</li><li>Mikkelsen, L., Buchakchiev, R., Madsen, T., &amp; Schwefel, H. P. (2016, September). Public transport occupancy estimation using WLAN probing. In 2016 8th International Workshop on Resilient Networks Design and Modeling (RNDM) (pp. 302-308). IEEE.</li><li>Afshari, H. H., Jalali, S., Ghods, A. H., &amp; Raahemi, B. (2018, November). An Intelligent Traffic Management System Based on the Wi-Fi and Bluetooth Sensing and Data Clustering. In Proceedings of the Future Technologies Conference (pp. 298-312). Springer, Cham.</li><li>Oransirikul, T., Piumarta, I., &amp; Takada, H. (2019). Classifying Passenger and Non-passenger Signals in Public Transportation by Analysing Mobile Device Wi-Fi Activity. Journal of Information Processing, 27, 25-32.</li><li>Ji, Y., Zhao, J., Zhang, Z., &amp; Du, Y. (2017). Estimating bus loads and OD flows using location-stamped farebox and Wi-Fi signal data. Journal of Advanced Transportation, 2017.</li><li>Håkegård, J. E., Myrvoll, T. A., &amp; Skoglund, T. R. (2018, November). Statistical modelling for estimation of OD matrices for public transport using Wi-Fi and APC data. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC) (pp. 1005-1010). IEEE.</li><li>Ben-Akiva, M., Macke, P. P., &amp; Hsu, P. S. (1985). Alternative methods to estimate route-level trip tables and expand on-board surveys. Transportation Research Record, (1037).</li><li>Cui, A. (2006). Bus passenger origin-destination matrix estimation using automated data collection systems (Doctoral dissertation, Massachusetts Institute of Technology).</li></ol><center>留言区欢迎任何的建议与批评<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8">💡</span>~</center><center>如果你觉得这篇文章对你有用，欢迎分享、打赏哦<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2615.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2615.png?v8">☕</span>~</center><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;em&gt;武汉加油！中国加油！&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;2020年第一篇博客，首先来填坑。本项目搭建了Wi-Fi检测信息分析网站，介绍了基于Wi-Fi信号检测的公交OD估计方法。这是本科两个毕设（软件工程+交通工程）的结合。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="本科毕设" scheme="http://www.caoxu.club/categories/%E6%9C%AC%E7%A7%91%E6%AF%95%E8%AE%BE/"/>
    
    
      <category term="Wi-Fi检测" scheme="http://www.caoxu.club/tags/Wi-Fi%E6%A3%80%E6%B5%8B/"/>
    
      <category term="OD矩阵估计" scheme="http://www.caoxu.club/tags/OD%E7%9F%A9%E9%98%B5%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="极大似然估计法" scheme="http://www.caoxu.club/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E6%B3%95/"/>
    
      <category term="JSP" scheme="http://www.caoxu.club/tags/JSP/"/>
    
  </entry>
  
  <entry>
    <title>改进CNN识别Fashion-MINST</title>
    <link href="http://www.caoxu.club/2020/01/03/%E6%94%B9%E8%BF%9BCNN%E8%AF%86%E5%88%ABFashion-MINST/"/>
    <id>http://www.caoxu.club/2020/01/03/改进CNN识别Fashion-MINST/</id>
    <published>2020-01-03T02:40:33.000Z</published>
    <updated>2020-01-03T02:41:02.052Z</updated>
    
    <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;script&gt;
        document.querySelectorAll(&#39;.github-emoji&#39;)
          .forEach(el =&gt; {
            if (!el.dataset.src) { return; }
        
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>R语言|逻辑回归模型建模案例（交通事故预测）</title>
    <link href="http://www.caoxu.club/2019/11/07/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>http://www.caoxu.club/2019/11/07/逻辑回归模型/</id>
    <published>2019-11-07T02:35:20.000Z</published>
    <updated>2020-05-04T01:30:25.228Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p><em>所有的模型都是错的，但是有用 <span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8">🌟</span></em></p></blockquote><img title="拟合ROC" data-src="/2019/11/07/逻辑回归模型/拟合ROC.png"><p>本文介绍如何建立逻辑回归模型进行交通事故预测，并验证了模型的准确度。<br><a id="more"></a></p><div class="note default">            <h2 id="一、问题描述与数据准备"><a href="#一、问题描述与数据准备" class="headerlink" title="一、问题描述与数据准备"></a>一、问题描述与数据准备</h2>          </div><h3 id="1-1-问题描述"><a href="#1-1-问题描述" class="headerlink" title="1.1 问题描述"></a>1.1 问题描述</h3><p>  基于交通事故相关数据，建立逻辑回归模型，并尽可能使模型的预测准确度达到最高。</p><p>  使用交通事故发生前的车辆速度变化值、道路流量的变化值和天气的类型（下雨/晴天），建立回归模型对是否发生交通事故（事故/非事故）进行预测。</p><h3 id="1-2-数据导入与类型转换"><a href="#1-2-数据导入与类型转换" class="headerlink" title="1.2 数据导入与类型转换"></a>1.2 数据导入与类型转换</h3><h4 id="1-2-1-数据读取"><a href="#1-2-1-数据读取" class="headerlink" title="1.2.1 数据读取"></a>1.2.1 数据读取</h4><p>原始数据可通过百度网盘下载：<span class="exturl" data-url="aHR0cHM6Ly9wYW4uYmFpZHUuY29tL3MvMXRlVk1pWU9hYUhqRVhsRFg1MjBNY3cg" title="https://pan.baidu.com/s/1teVMiYOaaHjEXlDX520Mcw "><i class="fa fa-download fa-fw"></i>数据下载<i class="fa fa-external-link"></i></span></p><p>使用<code>read.csv（）</code>函数读取CSV格式数据</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">SourceData &lt;- read.csv(<span class="string">"C:/Users/Fred/R_project/GLM_dataModel/data 1.csv"</span>)</span><br></pre></td></tr></tbody></table></figure><h4 id="1-2-2-数据类型转化"><a href="#1-2-2-数据类型转化" class="headerlink" title="1.2.2 数据类型转化"></a>1.2.2 数据类型转化</h4><p>原始数据类型如下表所示，需要将<code>case</code>和<code>weather</code>均转换为无序因子型的数据。</p><center>表1-1  原始数据分析</center><div class="table-container"><table><thead><tr><th><center>变量名</center></th><th><center>数据类型</center></th><th><center>变量描述</center></th></tr></thead><tbody><tr><td><center>case</center></td><td><center>int</center></td><td><center>是否发生事故，1：事故，0：非事故</center></td></tr><tr><td><center>spd_dif_1min</center></td><td><center> Num</center></td><td><center>事故发生前0-1分钟内的速度变化值</center></td></tr><tr><td><center>spd_dif_2min</center></td><td><center> Num</center></td><td><center>事故发生前1-2分钟内的速度变化值</center></td></tr><tr><td><center>spd_dif_3min</center></td><td><center> Num</center></td><td><center>事故发生前2-3分钟内的速度变化值</center></td></tr><tr><td><center>spd_dif_4min</center></td><td><center>Num</center></td><td><center>事故发生前3-4分钟内的速度变化值</center></td></tr><tr><td><center>vol_dif_1min</center></td><td><center>Num</center></td><td><center>事故发生前0-1分钟内的流量变化值</center></td></tr><tr><td><center>vol_dif_2min</center></td><td><center>Num</center></td><td><center>事故发生前1-2分钟内的流量变化值</center></td></tr><tr><td><center>vol_dif_3min</center></td><td><center> Num</center></td><td><center>事故发生前2-3分钟内的流量变化值</center></td></tr><tr><td><center>vol_dif_4min</center></td><td><center> Num</center></td><td><center>事故发生前3-4分钟内的流量变化值</center></td></tr><tr><td><center>Weather</center></td><td><center>int</center></td><td><center>1（下雨），0（晴天）</center></td></tr></tbody></table></div><p>使用<code>factor（）</code>函数实现类型转换，代码实现如下。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">## 将case、weather转换为因子型变量，表示类型</span></span><br><span class="line">nonNA_Data$case &lt;- factor(nonNA_Data$case,levels = c(<span class="number">0</span>,<span class="number">1</span>),labels = c(<span class="string">"非事故"</span>,<span class="string">"事故"</span>))</span><br><span class="line">nonNA_Data$Weather &lt;- factor(nonNA_Data$Weather,levels = c(<span class="number">0</span>,<span class="number">1</span>),labels = c(<span class="string">"晴天"</span>,<span class="string">"下雨"</span>))</span><br></pre></td></tr></tbody></table></figure><div class="note default">            <h2 id="二、数据预处理"><a href="#二、数据预处理" class="headerlink" title="二、数据预处理"></a>二、数据预处理</h2>          </div><h3 id="2-1-统计冗余数据"><a href="#2-1-统计冗余数据" class="headerlink" title="2.1 统计冗余数据"></a>2.1 统计冗余数据</h3><p>首先检查是否含有冗余数据，使用<code>duplicated（）</code>函数对原始数据进行判断，检查发现无重复数据。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">##冗余数据处理</span></span><br><span class="line">duplicated_count &lt;- sum(duplicated(SourceData))<span class="comment"># 结果无重复数据</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-2-缺失值分析及处理"><a href="#2-2-缺失值分析及处理" class="headerlink" title="2.2 缺失值分析及处理"></a>2.2 缺失值分析及处理</h3><h4 id="2-2-1-缺失值分析"><a href="#2-2-1-缺失值分析" class="headerlink" title="2.2.1 缺失值分析"></a>2.2.1 缺失值分析</h4><p>在得到数据集后，我们需要观察数据的分布情况，因为很多的模型对缺失值敏感，因此观察是否有缺失值是其中很重要的一个步骤。在正式分析前，我们先通过图形进行对观测字段的缺失情况有一个直观的感受。</p><p>  使用<strong>VIM</strong>包中的<code>aggr（）</code>函数对缺失值进行图形探究。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(<span class="string">"VIM"</span>)</span><br><span class="line">aggr(SourceData,prop=<span class="literal">FALSE</span>,numbers = <span class="literal">TRUE</span>)</span><br></pre></td></tr></tbody></table></figure><img title="aggr()生成的原始数据集的缺失值模式图形" data-src="/2019/11/07/逻辑回归模型/图2-1%20%20aggr()生成的原始数据集的缺失值模式图形.png"><center>图2-1  aggr()生成的原始数据集的缺失值模式图形</center><p><code>aggr()</code>函数不仅绘制每个变量的缺失值数，还绘制每个变量组合的缺失值数。红色表示缺失，不含缺失值的实例有759个，有缺失值的实例36个；可以看出前4分钟的速度变化值缺失值最多有11个。</p><p>使用<code>matrixplot()</code>函数可以生成每个实例数据的图形。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrixplot(SourceData)</span><br></pre></td></tr></tbody></table></figure><img title="图2-2  原始数据集按实例（行）展示真实值和缺失值的矩阵图" data-src="/2019/11/07/逻辑回归模型/图2-2%20%20原始数据集按实例（行）展示真实值和缺失值的矩阵图.png"><center>图2-2  原始数据集按实例（行）展示真实值和缺失值的矩阵图</center><p>此处，数值型数据被重新转换到[0, 1]区间，并用灰度来表示大小：浅色表示值小，深色表示值大。默认缺失值为红色</p><h4 id="2-2-2-缺失值插补"><a href="#2-2-2-缺失值插补" class="headerlink" title="2.2.2 缺失值插补"></a>2.2.2 缺失值插补</h4><p>对于缺失值的处理方法非常多，例如基于聚类的方法，基于回归的方法，基于均值的方法，其中最简单的方法是直接移除，但是在本文中因为缺失值所占比例较高，直接移除会损失大量观测，因此并不是最合适的方法。在这里，我们使用<code>KNN方法</code>对缺失值进行填补。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    <span class="comment">### 删除（异常值）case为空的记录</span></span><br><span class="line">nonNA_Data &lt;- SourceData[-which(is.na(SourceData$case)),]</span><br><span class="line">    <span class="comment">### 使用kNN方法对缺失值填补</span></span><br><span class="line">nonNA_Data &lt;- knnImputation(nonNA_Data,k=<span class="number">10</span>,meth = <span class="string">"weighAvg"</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-3-异常值分析及处理"><a href="#2-3-异常值分析及处理" class="headerlink" title="2.3 异常值分析及处理"></a>2.3 异常值分析及处理</h3><p>使用箱型图对各个单因素变量进行异常值的分析，结果如下，发现并无明显的异常值。</p><img title="图2-3  异常值分析" data-src="/2019/11/07/逻辑回归模型/图2-3%20%20异常值分析.png"><center>图2-3  异常值分析</center><div class="note default">            <h2 id="三、变量分析"><a href="#三、变量分析" class="headerlink" title="三、变量分析"></a>三、变量分析</h2>          </div><h3 id="3-1-单变量分析"><a href="#3-1-单变量分析" class="headerlink" title="3.1 单变量分析"></a>3.1 单变量分析</h3><p>主要分析各个变量的分布，使用<code>ggplot2</code>包中的<code>ggplot（）</code>函数绘制各变量的分布统计图如下图。为了简洁，只展示两个变量。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ggplot2)</span><br><span class="line">opar &lt;- par(no.readonly = <span class="literal">TRUE</span>)</span><br><span class="line">par(mfrow = c(<span class="number">2</span>,<span class="number">4</span>))</span><br><span class="line">ggplot(df.train, aes(x = spd_dif_1min, y = ..density..)) + geom_histogram(fill = <span class="string">"blue"</span>, colour = <span class="string">"grey60"</span>, size = <span class="number">0.2</span>, alpha = <span class="number">0.2</span>) + geom_density()</span><br><span class="line">ggplot(df.train, aes(x = spd_dif_2min, y = ..density..)) + geom_histogram(fill = <span class="string">"blue"</span>, colour = <span class="string">"grey60"</span>, size = <span class="number">0.2</span>, alpha = <span class="number">0.2</span>) + geom_density()</span><br></pre></td></tr></tbody></table></figure><img title="图3-1  变量分布统计图1" data-src="/2019/11/07/逻辑回归模型/图3-1%20%20变量分布统计图1.png"><img title="图3-1  变量分布统计图2" data-src="/2019/11/07/逻辑回归模型/图3-1%20%20变量分布统计图2.png"><center>图3-1  变量分布统计图</center><p>可以看出，各个变量的分布大致呈负指数分布。</p><h3 id="3-2-多变量之间的相关性分析"><a href="#3-2-多变量之间的相关性分析" class="headerlink" title="3.2 多变量之间的相关性分析"></a>3.2 多变量之间的相关性分析</h3><p>建模之前首先得检验变量之间的相关性，如果变量之间相关性显著，会影响模型的预测效果。下面通过<code>corrplot</code>函数，画出各变量之间，包括响应变量与自变量的相关性。由下图可以看出，各变量之间的相关性是非常小的。</p><img title="图3-2  变量之间的相关性分析" data-src="/2019/11/07/逻辑回归模型/图3-2%20%20变量之间的相关性分析.png"><center>图3-2  变量之间的相关性分析</center><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(corrplot)</span><br><span class="line">cor1&lt;-cor(nonNA_Data[,<span class="number">2</span>:<span class="number">9</span>])</span><br><span class="line">corrplot(cor1,method = <span class="string">"number"</span>)</span><br></pre></td></tr></tbody></table></figure><div class="note default">            <h2 id="四、Logistic回归建模"><a href="#四、Logistic回归建模" class="headerlink" title="四、Logistic回归建模"></a>四、Logistic回归建模</h2>          </div><h3 id="4-1-数据集分割"><a href="#4-1-数据集分割" class="headerlink" title="4.1 数据集分割"></a>4.1 数据集分割</h3><p>将样本分为按照模型标定：模型校验=7:3的比例划分。<code>nrow（x）</code>返回x对象的序号，<code>Sample（）</code>函数随机抽取70%的序号，由此完成数据分割。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train &lt;- sample(nrow(nonNA_Data), <span class="number">0.7</span>*nrow(nonNA_Data))</span><br><span class="line">  <span class="comment">## 表示从数据集中随机抽取70%的数据的序号</span></span><br><span class="line">df.train &lt;- nonNA_Data[train,]</span><br><span class="line">df.validate &lt;- nonNA_Data[-train,]</span><br></pre></td></tr></tbody></table></figure><h3 id="4-2-建立模型"><a href="#4-2-建立模型" class="headerlink" title="4.2 建立模型"></a>4.2 建立模型</h3><p>首先将所有自变量都考虑放入模型中，进行Logistic回归建模，模型如下。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">fit.full &lt;- glm(case~spd_dif_1min + spd_dif_2min + spd_dif_3min + spd_dif_4m</span><br><span class="line"><span class="keyword">in</span> + vol_dif_1min + vol_dif_2min + vol_dif_3min + vol_dif_4min + Weather, da</span><br><span class="line">ta = df.train, family = binomial(link = <span class="string">'logit'</span>))</span><br><span class="line"> summary(fit.full)</span><br><span class="line"> <span class="comment">## Call:</span></span><br><span class="line"> <span class="comment">## glm(formula = case ~ spd_dif_1min + spd_dif_2min + spd_dif_3min +</span></span><br><span class="line"> <span class="comment">## spd_dif_4min + vol_dif_1min + vol_dif_2min + vol_dif_3min +</span></span><br><span class="line"> <span class="comment">## vol_dif_4min + Weather, family = binomial(link = "logit"),</span></span><br><span class="line"> <span class="comment">## data = df.train)</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Deviance Residuals:</span></span><br><span class="line"> <span class="comment">## Min 1Q Median 3Q Max</span></span><br><span class="line"> <span class="comment">## -1.6603 -0.6247 -0.4932 -0.4031 2.2398</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Coefficients:</span></span><br><span class="line"> <span class="comment">## Estimate Std. Error z value Pr(&gt;|z|)</span></span><br><span class="line"> <span class="comment">## (Intercept) -2.93342 0.27208 -10.781 &lt; 2e-16 ***</span></span><br><span class="line"> <span class="comment">## spd_dif_1min 0.13201 0.03045 4.335 1.46e-05 ***</span></span><br><span class="line"> <span class="comment">## spd_dif_2min 0.02350 0.04661 0.504 0.6141</span></span><br><span class="line"> <span class="comment">## spd_dif_3min 0.06076 0.04150 1.464 0.1431</span></span><br><span class="line"> <span class="comment">## spd_dif_4min 0.09812 0.04155 2.361 0.0182 *</span></span><br><span class="line"> <span class="comment">## vol_dif_1min 0.02697 0.01127 2.394 0.0167 *</span></span><br><span class="line"> <span class="comment">## vol_dif_2min 0.02265 0.01301 1.742 0.0815 .</span></span><br><span class="line"> <span class="comment">## vol_dif_3min 0.01161 0.01170 0.993 0.3209</span></span><br><span class="line"> <span class="comment">## vol_dif_4min 0.01010 0.01142 0.884 0.3767</span></span><br><span class="line"> <span class="comment">## Weather 下雨 0.15360 0.37075 0.414 0.6787</span></span><br><span class="line"> <span class="comment">## ---</span></span><br><span class="line"> <span class="comment">## Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## (Dispersion parameter for binomial family taken to be 1)</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Null deviance: 544.17 on 554 degrees of freedom</span></span><br><span class="line"> <span class="comment">## Residual deviance: 487.01 on 545 degrees of freedom</span></span><br><span class="line"> <span class="comment">## AIC: 507.01</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Number of Fisher Scoring iterations: 4</span></span><br></pre></td></tr></tbody></table></figure><p>从<code>summary（）</code>返回的结果看出，自变量<code>spd_dif_1min</code>、<code>spd_dif_4min</code>和<code>vol_dif_1min</code>对方程的贡献度较为显著，而其他变量对方程的贡献度不显著，去除这些变量，检验新模型是否拟合得好：</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">fit.reduced &lt;- glm(case ~ spd_dif_1min + spd_dif_4min + vol_dif_1min, data =</span><br><span class="line">df.train, family = binomial(link = <span class="string">'logit'</span>))</span><br><span class="line"> summary(fit.reduced)</span><br><span class="line"></span><br><span class="line"> <span class="comment">## Call:</span></span><br><span class="line"> <span class="comment">## glm(formula = case ~ spd_dif_1min + spd_dif_4min + vol_dif_1min,</span></span><br><span class="line"> <span class="comment">## family = binomial(link = "logit"), data = df.train)</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Deviance Residuals:</span></span><br><span class="line"> <span class="comment">## Min 1Q Median 3Q Max</span></span><br><span class="line"> <span class="comment">## -1.8244 -0.6018 -0.5241 -0.4484 2.1127</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Coefficients:</span></span><br><span class="line"> <span class="comment">## Estimate Std. Error z value Pr(&gt;|z|)</span></span><br><span class="line"> <span class="comment">## (Intercept) -2.44831 0.20239 -12.097 &lt; 2e-16 ***</span></span><br><span class="line"> <span class="comment">## spd_dif_1min 0.13722 0.02938 4.670 3.01e-06 ***</span></span><br><span class="line"> <span class="comment">## spd_dif_4min 0.12250 0.03863 3.171 0.00152 **</span></span><br><span class="line"> <span class="comment">## vol_dif_1min 0.03197 0.01114 2.871 0.00410 **</span></span><br><span class="line"> <span class="comment">## ---</span></span><br><span class="line"> <span class="comment">## Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## (Dispersion parameter for binomial family taken to be 1)</span></span><br><span class="line"> <span class="comment">## Null deviance: 544.17 on 554 degrees of freedom</span></span><br><span class="line"> <span class="comment">## Residual deviance: 496.79 on 551 degrees of freedom</span></span><br><span class="line"> <span class="comment">## AIC: 504.79</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Number of Fisher Scoring iterations: 4</span></span><br></pre></td></tr></tbody></table></figure><p>新模型的每个回归系数都非常显著（<strong>p&lt;0.05</strong>）。由于两模型嵌套（<code>fit.reduced</code>是<code>fit.full</code>的一个子集），可以使用<code>anova()</code>函数对它们进行比较，对于广义线性回归，可用卡方检验。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">anova(fit.reduced, fit.full, test = <span class="string">"Chisq"</span>)</span><br><span class="line"> <span class="comment">## Analysis of Deviance Table</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">## Model 1: case ~ spd_dif_1min + spd_dif_4min + vol_dif_1min</span></span><br><span class="line"> <span class="comment">## Model 2: case ~ spd_dif_1min + spd_dif_2min + spd_dif_3min + spd_dif_4min+</span></span><br><span class="line"> <span class="comment">## vol_dif_1min + vol_dif_2min + vol_dif_3min + vol_dif_4min +</span></span><br><span class="line"> <span class="comment">## Weather</span></span><br><span class="line"> <span class="comment">## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)</span></span><br><span class="line"> <span class="comment">## 1 551 496.79</span></span><br><span class="line"> <span class="comment">## 2 545 487.01 6 9.7794 0.1343</span></span><br></pre></td></tr></tbody></table></figure><p>结果的卡方值不显著（<strong>p=0.1343</strong>），<strong>表明三个预测变量的新模型与九个完整预测变量的模型拟合程度一样好</strong>。添加其他6个变量不会显著提高方程的预测精度，因此可以依据更简单的模型进行解释。</p><p>解释模型参数：</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exp(coef(fit.reduced))</span><br><span class="line"><span class="comment">## (Intercept) spd_dif_1min spd_dif_4min vol_dif_1min</span></span><br><span class="line"><span class="comment">## 0.08643927 1.14708542 1.13031911 1.03248943</span></span><br></pre></td></tr></tbody></table></figure><p>可以看到，1min内速度的变化量增加1个单位，事故发生的优势将乘以1.14708542，同理1min速度、2min流量的变化量增加一个单位，事故发生的优势将乘以1.13031911、1.03248943。</p><h3 id="4-3-判定标定模型的拟合优度"><a href="#4-3-判定标定模型的拟合优度" class="headerlink" title="4.3 判定标定模型的拟合优度"></a>4.3 判定标定模型的拟合优度</h3><p>  通常一个二值分类器可以通过ROC（Receiver Operating Characteristic）曲线和AUC值来评价优劣。</p><p>  很多二元分类器会产生一个概率预测值，而非仅仅是0-1预测值。我们可以使用某个阈值（例如0.5），以划分哪些预测为1，哪些预测为0。得到二元预测值后，可以构建一个混淆矩阵来评价二元分类器的预测效果。所有的训练数据都会落入这个矩阵中，而对角线上的数字代表了预测正确的数目，即true positive + true negative。同时可以相应算出TPR（真正率或称为敏感度）和TNR（真负率或称为特异度）。我们主观上希望这两个指标越大越好，但可惜二者是一个此消彼涨的关系。除了分类器的训练参数，阈值的选择，也会大大的影响TPR和TNR。有时可以根据具体问题和需要，来选择具体的阈值。</p><img title="TPR和TNR" data-src="/2019/11/07/逻辑回归模型/TPR和TNR.png"><p>首先使用训练数据集生成概率预测值：</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(pROC)</span><br><span class="line"> pre &lt;- predict(fit.reduced,type=<span class="string">'response'</span>,df.train)</span><br></pre></td></tr></tbody></table></figure><p>需要特别注意的是：<code>type</code>一定要加，否则计算的最优阈值为负数</p><p>下面使用<code>pROC</code>包计算<code>fit.reduced</code>模型的拟合优度：</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">modelroc &lt;- roc(df.train$case,pre)</span><br><span class="line">plot(modelroc, print.auc=<span class="literal">TRUE</span>, auc.polygon=<span class="literal">TRUE</span>, </span><br><span class="line">     grid=c(<span class="number">0.1</span>, <span class="number">0.2</span>), grid.col=c(<span class="string">"green"</span>, <span class="string">"red"</span>),</span><br><span class="line">     max.auc.polygon=<span class="literal">TRUE</span>,</span><br><span class="line">     auc.polygon.col=<span class="string">"skyblue"</span>, print.thres=<span class="literal">TRUE</span>)</span><br></pre></td></tr></tbody></table></figure><img title="图4-1  拟合优度ROC" data-src="/2019/11/07/逻辑回归模型/图4-1%20%20拟合优度ROC.png"><center>图4-1  拟合优度ROC</center><p>从图中可以看出阈值为<code>0.142</code>时，对应的AUC（Area Under Curve）值为<code>0.762</code>大于0.7，区分能力可以接受，还有提高的空间。阈值为0.142，对应的敏感度和特异度分别为：<code>0.907</code>、<code>0.533</code>。</p><div class="note default">            <h2 id="五、评估模型的预测效果"><a href="#五、评估模型的预测效果" class="headerlink" title="五、评估模型的预测效果"></a>五、评估模型的预测效果</h2>          </div><p>基于测试集使用建立的模型计算预测值，与真值比较计算出预测的准确率：</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 评估模型的预测效果</span></span><br><span class="line"> predict &lt;- predict(fit.reduced,type=<span class="string">'response'</span>,newdata=df.validate)</span><br><span class="line"> predict.results &lt;- ifelse( predict&gt; <span class="number">0.142</span>,<span class="string">"事故"</span>,<span class="string">"非事故"</span>)</span><br><span class="line"> <span class="comment">## 错误率</span></span><br><span class="line"> misClasificError &lt;- mean(predict.results != df.validate$case)</span><br><span class="line"> print(paste(<span class="string">'准确率'</span>,<span class="number">1</span>-misClasificError))</span><br></pre></td></tr></tbody></table></figure><p>计算的准确率为 <strong>0.62343</strong>，准确率不是很高<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f648.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f648.png?v8">🙈</span>，可以考虑重新设计模型的结构提高准确率。</p><p>使用测试数据集计算ROC拟合优度，对于测试数据集AUC为0.765，TPR为0.923，TNR为0.588。</p><figure class="highlight r"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试集的ROC拟合优度</span></span><br><span class="line">validate_pre &lt;- predict(fit.reduced,type=<span class="string">'response'</span>,df.validate)</span><br><span class="line">validate_roc &lt;- roc(df.validate$case,validate_pre)</span><br><span class="line">plot(validate_roc, print.auc=<span class="literal">TRUE</span>, auc.polygon=<span class="literal">TRUE</span>, grid=c(<span class="number">0.1</span>, <span class="number">0.2</span>),</span><br><span class="line">     grid.col=c(<span class="string">"green"</span>, <span class="string">"red"</span>), max.auc.polygon=<span class="literal">TRUE</span>,</span><br><span class="line">     auc.polygon.col=<span class="string">"skyblue"</span>, print.thres=<span class="literal">TRUE</span>)</span><br><span class="line">  <span class="comment">## AUC = 0.765, TPR = 0.923, TNR = 0.588,阈值 = 0.149</span></span><br></pre></td></tr></tbody></table></figure><img title="图5-1  测试数据集拟合优度" data-src="/2019/11/07/逻辑回归模型/图5-1%20%20测试数据集拟合优度.png"><center>图5-1  测试数据集拟合优度</center><div class="note success">            <h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>  通过这个案例对广义线性模型中的logistics回归模型有了初步的了解,练习了数据预处理（建模准备）、建立模型、评价模型的完整过程<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png?v8">😎</span>。</p>          </div><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Kabacoff R , 卡巴科弗, Kabacoff, et al. <span class="exturl" data-url="aHR0cHM6Ly9wYW4uYmFpZHUuY29tL3MvMXpXdHE=" title="https://pan.baidu.com/s/1zWtq">R 语言实战<i class="fa fa-external-link"></i></span>[M]. 人民邮电出版社, 2013.</p><p>[2] <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vbnhsZC9wLzYxNzA2OTAuaHRtbA==" title="https://www.cnblogs.com/nxld/p/6170690.html">如何在R语言中使用Logistic回归模型<i class="fa fa-external-link"></i></span></p><p>[3] <span class="exturl" data-url="aHR0cHM6Ly93d3cuY25ibG9ncy5jb20vTGl0dGxlQm9vay9wLzY4NDI5MjAuaHRtbA==" title="https://www.cnblogs.com/LittleBook/p/6842920.html">信用卡评分<i class="fa fa-external-link"></i></span></p><p>[4] <span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC8zMDk4MjU1MQ==" title="https://zhuanlan.zhihu.com/p/30982551">logistic回归预测婚姻出轨|R语言<i class="fa fa-external-link"></i></span></p><center>留言区欢迎任何的建议与批评<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png?v8">💡</span>~</center><center>如果你觉得这篇文章对你有用，欢迎分享、打赏哦<span class="github-emoji" style="color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/2615.png?v8) center/contain" data-src="https://github.githubassets.com/images/icons/emoji/unicode/2615.png?v8">☕</span>~</center><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;&lt;em&gt;所有的模型都是错的，但是有用 &lt;span class=&quot;github-emoji&quot; style=&quot;color: transparent;background:no-repeat url(https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8) center/contain&quot; data-src=&quot;https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8&quot;&gt;🌟&lt;/span&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;img title=&quot;拟合ROC&quot; data-src=&quot;/2019/11/07/逻辑回归模型/拟合ROC.png&quot;&gt;
&lt;p&gt;本文介绍如何建立逻辑回归模型进行交通事故预测，并验证了模型的准确度。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2019秋《交通数据分析》" scheme="http://www.caoxu.club/categories/2019%E7%A7%8B%E3%80%8A%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B/"/>
    
      <category term="广义线性模型" scheme="http://www.caoxu.club/categories/2019%E7%A7%8B%E3%80%8A%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="logistics回归模型" scheme="http://www.caoxu.club/categories/2019%E7%A7%8B%E3%80%8A%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/logistics%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="R" scheme="http://www.caoxu.club/tags/R/"/>
    
      <category term="广义线性模型" scheme="http://www.caoxu.club/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="logistic回归模型" scheme="http://www.caoxu.club/tags/logistic%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="数据预处理" scheme="http://www.caoxu.club/tags/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
      <category term="拟合优度" scheme="http://www.caoxu.club/tags/%E6%8B%9F%E5%90%88%E4%BC%98%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>BOSS直聘“自动驾驶”岗位信息爬取与分析</title>
    <link href="http://www.caoxu.club/2019/10/17/BOSS%E7%9B%B4%E8%81%98%E7%88%AC%E8%99%AB/"/>
    <id>http://www.caoxu.club/2019/10/17/BOSS直聘爬虫/</id>
    <published>2019-10-17T07:40:49.000Z</published>
    <updated>2019-10-21T01:49:46.160Z</updated>
    
    <content type="html"><![CDATA[<p>  这是我个人博客的第一篇文章，是全新的开始。希望未来在这里记录更多有趣的探索。</p><img title="自动驾驶岗位名称" data-src="/2019/10/17/BOSS直聘爬虫/岗位名称.jpg"><p>  自动驾驶随着人工智能技术的成熟，人才需求在不断增加，为了对我国自动驾驶人才需求进行了解，本文选择<span class="exturl" data-url="aHR0cHM6Ly93d3cuemhpcGluLmNvbQ==" title="https://www.zhipin.com">BOSS直聘<i class="fa fa-external-link"></i></span>招聘网站获取自动驾驶相关职位信息，分析数据，根据分析结果确定人才需求、为以后的学习制定计划。<br><a id="more"></a></p><p>本文开发环境：<span class="exturl" data-url="aHR0cHM6Ly93d3cucHl0aG9uLm9yZw==" title="https://www.python.org">python3.7<i class="fa fa-external-link"></i></span> 、<span class="exturl" data-url="aHR0cHM6Ly93d3cuYW5hY29uZGEuY29t" title="https://www.anaconda.com">anconda3<i class="fa fa-external-link"></i></span></p><h2 id="一、数据爬取"><a href="#一、数据爬取" class="headerlink" title="一、数据爬取"></a>一、数据爬取</h2><h3 id="1-1-分析目标页，构造URL"><a href="#1-1-分析目标页，构造URL" class="headerlink" title="1.1 分析目标页，构造URL"></a>1.1 分析目标页，构造URL</h3><p>  首先，进入网站输入关键字“自动驾驶”查询得到职位的列表页面，对目标页面的URL分析<code>https://www.zhipin.com/c101010100/?query=自动驾驶&amp;page=1&amp;ka=page-1</code>，发现关键的字段有：<code>query</code>、<code>page</code>分别代表了要查询的<strong>关键字</strong>和<strong>页面</strong>，<code>c101010100</code>代表了城市的编号，当选择了<strong>全国</strong>时编号为<code>c101010100</code>，所以通过控制这三个参数，实现对全国所有职位列表页的信息的爬取。</p><img title="目标页面" data-src="/2019/10/17/BOSS直聘爬虫/图1-1目标页面.png"><center>图1-1  目标页面</center><p>  其次，构建headers cookie，通过尝试发现爬取Boss直聘必须添加cookie，构建的请求头如下。</p><p><code>header = {         'User-Agent':            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36',         'Cookie':            '_uab_collina=156967238793885639512262；bannerClose_wanmeishijie=true; lastCity=101010100; _bl_uid=q5k7F1m8aOIjLLybtrIhbILink0X; __c=1570153169; __g=-;Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1569912508,1569985933,1570068443,1570153169;__l=l=%2Fwww.zhipin.com%2F&amp;r=&amp;friend_source=0&amp;friend_source=0;toUrl=https%3A%2F%2Fwww.zhipin.com%2Fjob_detail%2F%3Fquery%3D%25E8%2587%25AA%25E5%258A%25A8%25E9%25A9%25BE%25E9%25A9%25B6%26city%3D101020100%26industry%3D%26position%3D;__zp_stoken__=eaf4AHdTnnRrM4tosXCE%2FCPA2IBqs%2BoMIR%2FkL2KsnUmYiwLERJs40PPb8KIurKugecg3l0jmgV1zyN86oQkjtd81dw%3D%3D;Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1570159163;__a=64007787.1569672388.1570068443.1570153169.178.6.23.172;__zp_sseed__=4qNlVc1nPWG9Y7G66QfbgtQFW6iSHevalYp+HzrGFH4=; __zp_sname__=9d9e2c78;__zp_sts__=1570159388399'        }</code></p><h3 id="1-2-爬取、解析数据并保存"><a href="#1-2-爬取、解析数据并保存" class="headerlink" title="1.2 爬取、解析数据并保存"></a>1.2 爬取、解析数据并保存</h3><h4 id="1-2-1-确定要爬取的信息"><a href="#1-2-1-确定要爬取的信息" class="headerlink" title="1.2.1 确定要爬取的信息"></a>1.2.1 确定要爬取的信息</h4><p>  F12打开开发者工具，审查网页元素，需要获取的信息包括“工作名、薪资、工作地点、工作经验、学历要求、企业名、企业所属行业”。</p><img title="目标页面" data-src="/2019/10/17/BOSS直聘爬虫/图1-2分析目标页面.png"><center>图1-2  分析目标页面</center><h4 id="1-2-2-发送请求、解析数据、保存数据"><a href="#1-2-2-发送请求、解析数据、保存数据" class="headerlink" title="1.2.2 发送请求、解析数据、保存数据"></a>1.2.2 发送请求、解析数据、保存数据</h4><p>  对目标页面发送请求后，解析返回的html并提取所需要的信息，这里主要用到的包有：<code>urllib</code>、<code>requests</code>、<code>BeautifulSoup</code>、<code>lxml</code>。函数的主要步骤为：</p><ul><li>第一步，请求目标页面html</li><li>第二步，使用<code>lxml</code>和<code>bs4</code>解析目标信息</li><li>第三步，随机等待几秒，防止ip被封</li></ul><p>循环完成对所有页面岗位信息的提取，当完成数据的解析后，将数据保存到Excel表格中。下面的代码实现上述发送请求、解析数据、保存数据步骤。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_detial</span><span class="params">(total_page_count)</span>:</span></span><br><span class="line">    <span class="comment">#循环所有页面，</span></span><br><span class="line">    <span class="comment">#第一步，构造URL获取html</span></span><br><span class="line">    <span class="comment">#第二步，使用bs4解析详情页的超链接，保存到数组中</span></span><br><span class="line">    key_words = <span class="string">"自动驾驶"</span></span><br><span class="line">    key = urllib.parse.quote(key_words) <span class="comment">#将中文进行转码</span></span><br><span class="line">    jobs_title_list = []</span><br><span class="line">    area_list = []</span><br><span class="line">    work_exp_list = []</span><br><span class="line">    edu_list = []</span><br><span class="line">    company_list = []</span><br><span class="line">    salary_list = []</span><br><span class="line">    industry_list = []</span><br><span class="line">    financing_stage_list = []</span><br><span class="line">    company_scale_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,total_page_count+<span class="number">1</span>):   <span class="comment">#查看页面有10页职位列表</span></span><br><span class="line">        print(<span class="string">"正在爬取第 %s 页列表页..."</span> % i)</span><br><span class="line">        url =<span class="string">'https://www.zhipin.com/c100010000/?query='</span>+key+<span class="string">'&amp;page='</span>+str(i)+<span class="string">'&amp;ka=page-'</span>+str(i)</span><br><span class="line">        <span class="comment">#获取每页的列表html</span></span><br><span class="line">        per_page_html = get_job_html(url) </span><br><span class="line">        <span class="comment">#解析提取详情页超的数据</span></span><br><span class="line">        jobs_title,area,work_exp,edu,company,salary,industry\</span><br><span class="line">            =  parse_job_detial(per_page_html) <span class="comment">#,company_scale,financing_stage</span></span><br><span class="line">        <span class="comment"># 将每页30个数据尾加到总的数组</span></span><br><span class="line">        jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list\</span><br><span class="line">            = append_job_detial(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list,\</span><br><span class="line">                jobs_title,area,work_exp,edu,company,salary,industry) </span><br><span class="line">        <span class="comment">#随机等待,防止封ip</span></span><br><span class="line">        span=round(random.random()*<span class="number">6</span>,<span class="number">1</span>)</span><br><span class="line">        time.sleep(span)</span><br><span class="line">    print(<span class="string">"爬取列表页完成！"</span>)</span><br><span class="line">    <span class="comment"># 将爬取的数据保存到Excel</span></span><br><span class="line">    jobs_data = list(zip(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list)) </span><br><span class="line">    jobs_data.insert(<span class="number">0</span>, (<span class="string">"岗位"</span>,<span class="string">"工作地点"</span>,<span class="string">"工作经验要求"</span>,<span class="string">"学历要求"</span>,<span class="string">"招聘公司"</span>,<span class="string">"工资"</span>,<span class="string">"所属行业"</span>))</span><br><span class="line">    save_to_excel(<span class="string">"jobData"</span>, <span class="string">"AutoDriving"</span>, jobs_data) <span class="comment">#fileName,SheetName,数据</span></span><br><span class="line">    print(<span class="string">"保存数据成功，打开jobData.xls查看"</span>)</span><br></pre></td></tr></tbody></table></figure><p>  <code>get_job_html(url)</code>函数实现请求目标<code>url</code>，代码实现如下。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_job_html</span><span class="params">(url)</span>:</span></span><br><span class="line">    header = {</span><br><span class="line">         <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'</span>,</span><br><span class="line">         <span class="string">'Cookie'</span>:<span class="string">'_uab_collina=156967238793885639512262; bannerClose_wanmeishijie=true; lastCity=101010100; _bl_uid=q5k7F1m8aOIjLLybtrIhbILink0X; __c=1570153169; __g=-; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1569912508,1569985933,1570068443,1570153169; __l=l=%2Fwww.zhipin.com%2F&amp;r=&amp;friend_source=0&amp;friend_source=0; toUrl=https%3A%2F%2Fwww.zhipin.com%2Fjob_detail%2F%3Fquery%3D%25E8%2587%25AA%25E5%258A%25A8%25E9%25A9%25BE%25E9%25A9%25B6%26city%3D101020100%26industry%3D%26position%3D; __zp_stoken__=eaf4AHdTnnRrM4tosXCE%2FCPA2IBqs%2BoMIR%2FkL2KsnUmYiwLERJs40PPb8KIurKugecg3l0jmgV1zyN86oQkjtd81dw%3D%3D; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1570159163; __a=64007787.1569672388.1570068443.1570153169.178.6.23.172; __zp_sseed__=4qNlVc1nPWG9Y7G66QfbgtQFW6iSHevalYp+HzrGFH4=; __zp_sname__=9d9e2c78; __zp_sts__=1570159388399'</span></span><br><span class="line">         }</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        html = requests.get(url,headers = header)</span><br><span class="line">        <span class="keyword">if</span> html.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> html.text <span class="comment">#返回列表页的html</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure><p>  获取到网页html后解析想要的数据，实现代码如下。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_job_detial</span><span class="params">(per_page_html)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> per_page_html==<span class="string">''</span> <span class="keyword">or</span> len(per_page_html)==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"未请求到列表页的html！"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        jobs_title = []</span><br><span class="line">        area = []</span><br><span class="line">        work_exp = []</span><br><span class="line">        edu = []</span><br><span class="line">        company = []</span><br><span class="line">        salary = []</span><br><span class="line">        industry = []</span><br><span class="line">        financing_stage = []</span><br><span class="line">        company_scale = []</span><br><span class="line">        <span class="comment"># 初始化 标准化html</span></span><br><span class="line">        html = etree.HTML(per_page_html) </span><br><span class="line">        <span class="comment"># 工作名</span></span><br><span class="line">        jobs_title = html.xpath(<span class="string">'//div[@class="job-title"]/text()'</span>)</span><br><span class="line">        <span class="comment">#print(jobs_title)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment"># 工作地点</span></span><br><span class="line">        area = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[1]'</span>)</span><br><span class="line">        <span class="comment">#print(area)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#工作经验要求</span></span><br><span class="line">        work_exp = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[2]'</span>)</span><br><span class="line">        <span class="comment">#print(work_exp)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#学历要求</span></span><br><span class="line">        edu = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[3]'</span>)</span><br><span class="line">        <span class="comment">#print(edu)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#公司名</span></span><br><span class="line">        company = html.xpath(<span class="string">'//div[@class="company-text"]//a/text()'</span>)</span><br><span class="line">        <span class="comment">#print(company)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment"># 工资</span></span><br><span class="line">        salary = html.xpath(<span class="string">'//div[@class="info-primary"]//span[@class="red"]/text()'</span>)</span><br><span class="line">        <span class="comment">#print(salary)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#所属行业</span></span><br><span class="line">        industry = html.xpath(<span class="string">'//div[@class="info-company"]//p/text()[1]'</span>)</span><br><span class="line">        <span class="keyword">return</span> jobs_title,area,work_exp,edu,company,salary,industry</span><br></pre></td></tr></tbody></table></figure><p>  每次获取的是一页职位列表信息，需要将每次的数据尾加到总的数据<code>list</code>中，代码实现如下。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_job_detial</span><span class="params">(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list,jobs_title,area,work_exp,edu,company,salary,industry)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i,info <span class="keyword">in</span> enumerate(jobs_title):</span><br><span class="line">        jobs_title_list.append(jobs_title[i])</span><br><span class="line">        area_list.append(area[i])</span><br><span class="line">        work_exp_list.append(work_exp[i])</span><br><span class="line">        edu_list.append(edu[i])</span><br><span class="line">        company_list.append(company[i])</span><br><span class="line">        salary_list.append(salary[i])</span><br><span class="line">        industry_list.append(industry[i])</span><br><span class="line">    <span class="keyword">return</span> jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list</span><br></pre></td></tr></tbody></table></figure><p>  当所有页面的数据获取解析完成后，将数据保存为Excel的形式，代码实现：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_excel</span><span class="params">(filename,sheet_name,data)</span>:</span></span><br><span class="line">    f = xlwt.Workbook(encoding=<span class="string">'utf-8'</span>)  <span class="comment">#创建一个Workbook 设置编码</span></span><br><span class="line">    <span class="comment"># 第二参数表示是否可以覆盖单元格 其实是 Workbook实例化的一个参数，默认值为False</span></span><br><span class="line">    sheet = f.add_sheet(sheet_name,cell_overwrite_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，</span></span><br><span class="line">    <span class="comment"># 同时列出数据和数据下标，一般用在 for 循环当中。</span></span><br><span class="line">    <span class="keyword">for</span> row,row_data <span class="keyword">in</span> enumerate(data):  <span class="comment">#处理行</span></span><br><span class="line">        <span class="keyword">for</span> column,column_data <span class="keyword">in</span> enumerate(row_data): <span class="comment">#处理列</span></span><br><span class="line">            sheet.write(row,column,str(column_data))</span><br><span class="line">    f.save(filename + <span class="string">".xls"</span>)</span><br></pre></td></tr></tbody></table></figure><p>  对整个过程的代码集成如下。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree <span class="comment">#xpath</span></span><br><span class="line"><span class="keyword">from</span> requests.exceptions <span class="keyword">import</span> RequestException</span><br><span class="line"><span class="keyword">import</span> xlwt       <span class="comment">#excel操作</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_job_html</span><span class="params">(url)</span>:</span></span><br><span class="line">    header = {</span><br><span class="line">         <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36'</span>,</span><br><span class="line">         <span class="string">'Cookie'</span>:<span class="string">'_uab_collina=156967238793885639512262; bannerClose_wanmeishijie=true; lastCity=101010100; _bl_uid=q5k7F1m8aOIjLLybtrIhbILink0X; __c=1570153169; __g=-; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1569912508,1569985933,1570068443,1570153169; __l=l=%2Fwww.zhipin.com%2F&amp;r=&amp;friend_source=0&amp;friend_source=0; toUrl=https%3A%2F%2Fwww.zhipin.com%2Fjob_detail%2F%3Fquery%3D%25E8%2587%25AA%25E5%258A%25A8%25E9%25A9%25BE%25E9%25A9%25B6%26city%3D101020100%26industry%3D%26position%3D; __zp_stoken__=eaf4AHdTnnRrM4tosXCE%2FCPA2IBqs%2BoMIR%2FkL2KsnUmYiwLERJs40PPb8KIurKugecg3l0jmgV1zyN86oQkjtd81dw%3D%3D; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1570159163; __a=64007787.1569672388.1570068443.1570153169.178.6.23.172; __zp_sseed__=4qNlVc1nPWG9Y7G66QfbgtQFW6iSHevalYp+HzrGFH4=; __zp_sname__=9d9e2c78; __zp_sts__=1570159388399'</span></span><br><span class="line">         }</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        html = requests.get(url,headers = header)</span><br><span class="line">        <span class="keyword">if</span> html.status_code == <span class="number">200</span>:</span><br><span class="line">            <span class="keyword">return</span> html.text <span class="comment">#返回列表页的html</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">except</span> RequestException:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_job_detial</span><span class="params">(per_page_html)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> per_page_html==<span class="string">''</span> <span class="keyword">or</span> len(per_page_html)==<span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"未请求到列表页的html！"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        jobs_title = []</span><br><span class="line">        area = []</span><br><span class="line">        work_exp = []</span><br><span class="line">        edu = []</span><br><span class="line">        company = []</span><br><span class="line">        salary = []</span><br><span class="line">        industry = []</span><br><span class="line">        financing_stage = []</span><br><span class="line">        company_scale = []</span><br><span class="line">        <span class="comment"># 初始化 标准化html</span></span><br><span class="line">        html = etree.HTML(per_page_html) </span><br><span class="line">        <span class="comment"># 工作名</span></span><br><span class="line">        jobs_title = html.xpath(<span class="string">'//div[@class="job-title"]/text()'</span>)</span><br><span class="line">        <span class="comment">#print(jobs_title)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment"># 工作地点</span></span><br><span class="line">        area = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[1]'</span>)</span><br><span class="line">        <span class="comment">#print(area)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#工作经验要求</span></span><br><span class="line">        work_exp = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[2]'</span>)</span><br><span class="line">        <span class="comment">#print(work_exp)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#学历要求</span></span><br><span class="line">        edu = html.xpath(<span class="string">'//div[@class="info-primary"]//p/text()[3]'</span>)</span><br><span class="line">        <span class="comment">#print(edu)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#公司名</span></span><br><span class="line">        company = html.xpath(<span class="string">'//div[@class="company-text"]//a/text()'</span>)</span><br><span class="line">        <span class="comment">#print(company)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment"># 工资</span></span><br><span class="line">        salary = html.xpath(<span class="string">'//div[@class="info-primary"]//span[@class="red"]/text()'</span>)</span><br><span class="line">        <span class="comment">#print(salary)</span></span><br><span class="line">        <span class="comment">#a = 1</span></span><br><span class="line">        <span class="comment">#所属行业</span></span><br><span class="line">        industry = html.xpath(<span class="string">'//div[@class="info-company"]//p/text()[1]'</span>)</span><br><span class="line">        <span class="keyword">return</span> jobs_title,area,work_exp,edu,company,salary,industry</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">append_job_detial</span><span class="params">(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list,jobs_title,area,work_exp,edu,company,salary,industry)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i,info <span class="keyword">in</span> enumerate(jobs_title):</span><br><span class="line">        jobs_title_list.append(jobs_title[i])</span><br><span class="line">        area_list.append(area[i])</span><br><span class="line">        work_exp_list.append(work_exp[i])</span><br><span class="line">        edu_list.append(edu[i])</span><br><span class="line">        company_list.append(company[i])</span><br><span class="line">        salary_list.append(salary[i])</span><br><span class="line">        industry_list.append(industry[i])</span><br><span class="line">    <span class="keyword">return</span> jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_to_excel</span><span class="params">(filename,sheet_name,data)</span>:</span></span><br><span class="line">    f = xlwt.Workbook(encoding=<span class="string">'utf-8'</span>)  <span class="comment">#创建一个Workbook 设置编码</span></span><br><span class="line">    <span class="comment"># 第二参数表示是否可以覆盖单元格 其实是 Workbook实例化的一个参数，默认值为False</span></span><br><span class="line">    sheet = f.add_sheet(sheet_name,cell_overwrite_ok=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># enumerate() 函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，</span></span><br><span class="line">    <span class="comment"># 同时列出数据和数据下标，一般用在 for 循环当中。</span></span><br><span class="line">    <span class="keyword">for</span> row,row_data <span class="keyword">in</span> enumerate(data):  <span class="comment">#处理行</span></span><br><span class="line">        <span class="keyword">for</span> column,column_data <span class="keyword">in</span> enumerate(row_data): <span class="comment">#处理列</span></span><br><span class="line">            sheet.write(row,column,str(column_data))</span><br><span class="line">    f.save(filename + <span class="string">".xls"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_detial</span><span class="params">(total_page_count)</span>:</span></span><br><span class="line">    <span class="comment">#循环所有页面，</span></span><br><span class="line">    <span class="comment">#第一步，构造URL获取html</span></span><br><span class="line">    <span class="comment">#第二步，使用bs4解析详情页的超链接，保存到数组中</span></span><br><span class="line">    key_words = <span class="string">"自动驾驶"</span></span><br><span class="line">    key = urllib.parse.quote(key_words) <span class="comment">#将中文进行转码</span></span><br><span class="line">    jobs_title_list = []</span><br><span class="line">    area_list = []</span><br><span class="line">    work_exp_list = []</span><br><span class="line">    edu_list = []</span><br><span class="line">    company_list = []</span><br><span class="line">    salary_list = []</span><br><span class="line">    industry_list = []</span><br><span class="line">    financing_stage_list = []</span><br><span class="line">    company_scale_list = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,total_page_count+<span class="number">1</span>):   <span class="comment">#查看页面有10页职位列表</span></span><br><span class="line">        print(<span class="string">"正在爬取第 %s 页列表页..."</span> % i)</span><br><span class="line">        url =<span class="string">'https://www.zhipin.com/c100010000/?query='</span>+key+<span class="string">'&amp;page='</span>+str(i)+<span class="string">'&amp;ka=page-'</span>+str(i)</span><br><span class="line">        <span class="comment">#获取每页的列表html</span></span><br><span class="line">        per_page_html = get_job_html(url) </span><br><span class="line">        <span class="comment">#解析提取详情页超的数据</span></span><br><span class="line">        jobs_title,area,work_exp,edu,company,salary,industry\</span><br><span class="line">            =  parse_job_detial(per_page_html) <span class="comment">#,company_scale,financing_stage</span></span><br><span class="line">        <span class="comment"># 将每页30个数据尾加到总的数组</span></span><br><span class="line">        jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list\</span><br><span class="line">            = append_job_detial(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list,\</span><br><span class="line">                jobs_title,area,work_exp,edu,company,salary,industry) <span class="comment">#,financing_stage_list,financing_stage,financing_stage_list,company_scale,,company_scale_list,company_scale_list,</span></span><br><span class="line">        <span class="comment">#随机等待,防止封ip</span></span><br><span class="line">        span=round(random.random()*<span class="number">6</span>,<span class="number">1</span>)</span><br><span class="line">        time.sleep(span)</span><br><span class="line">    print(<span class="string">"爬取列表页完成！"</span>)</span><br><span class="line">    <span class="comment"># 将爬取的数据保存到Excel</span></span><br><span class="line">    jobs_data = list(zip(jobs_title_list,area_list,work_exp_list,edu_list,company_list,salary_list,industry_list)) <span class="comment">#,company_scale_list</span></span><br><span class="line">    jobs_data.insert(<span class="number">0</span>, (<span class="string">"岗位"</span>,<span class="string">"工作地点"</span>,<span class="string">"工作经验要求"</span>,<span class="string">"学历要求"</span>,<span class="string">"招聘公司"</span>,<span class="string">"工资"</span>,<span class="string">"所属行业"</span>))  <span class="comment">#,"公司规模"</span></span><br><span class="line">    save_to_excel(<span class="string">"jobData"</span>, <span class="string">"AutoDriving"</span>, jobs_data) <span class="comment">#fileName,SheetName,数据</span></span><br><span class="line">    print(<span class="string">"保存数据成功，打开jobData.xls查看"</span>)</span><br><span class="line"></span><br><span class="line">total_page_count = <span class="number">10</span> <span class="comment">#设置爬取列表页页数，10页</span></span><br><span class="line">get_all_detial(total_page_count) <span class="comment">#获取工作的数据保存到Excel</span></span><br></pre></td></tr></tbody></table></figure><img title="获取的数据" data-src="/2019/10/17/BOSS直聘爬虫/图1-4获取的数据.png"><center>图1-4  获取的数据</center><p>  图1-4为获取的目标数据，一共抓取了120个岗位信息。下一步就是对获取的信息进行清洗和统计分析。</p><h2 id="二、数据分析"><a href="#二、数据分析" class="headerlink" title="二、数据分析"></a>二、数据分析</h2><p>  本节主要介绍我们关心哪些信息，并如何对这些的数据进行分析处理和可视化，从而从这些数据中提取出有价值的信息。</p><h3 id="2-1-工作地点全国分布"><a href="#2-1-工作地点全国分布" class="headerlink" title="2.1 工作地点全国分布"></a>2.1 工作地点全国分布</h3><p>  找工作首先关注的点应该是“在哪”工作，找到一个房价便宜工资很高的工作应该是绝大多数求职者的最佳选择。那么“自动驾驶”相关岗位在全国范围内的分布是什么样的情况，本文通过使用优秀的可视化包pyecharts<sup>[1]</sup>实现对招聘岗位全国的分布情况的分析。</p><p><strong>在制作过程中pyecharts的<span class="exturl" data-url="aHR0cHM6Ly9weWVjaGFydHMub3JnLyMvemgtY24v" title="https://pyecharts.org/#/zh-cn/">官方文档<i class="fa fa-external-link"></i></span>起到了很大的作用，以后在开发过程中要重视官方文档的使用。</strong></p><img title="图2-1 自动驾驶职位招聘全国分布" data-src="/2019/10/17/BOSS直聘爬虫/图2-1%20自动驾驶职位招聘全国分布.png"><center>图2-1 自动驾驶职位招聘全国分布</center><img title="图2-2  自动驾驶岗位数量全国TOP10城市" data-src="/2019/10/17/BOSS直聘爬虫/图2-2%20%20自动驾驶岗位数量全国TOP10城市.png"><center>图2-2  自动驾驶岗位数量全国TOP10城市</center><p>代码实现</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Geo</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Bar</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyecharts.globals <span class="keyword">import</span> GeoType</span><br><span class="line"><span class="keyword">from</span> pyecharts.render <span class="keyword">import</span> make_snapshot</span><br><span class="line"><span class="keyword">from</span> snapshot_selenium <span class="keyword">import</span> snapshot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">city_counter</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    全国工作分布热力图</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    datas_copy[<span class="string">'工作地点'</span>] = datas_copy[<span class="string">'工作地点'</span>].apply(<span class="keyword">lambda</span> x :x.split(<span class="string">' '</span>)[<span class="number">0</span>])</span><br><span class="line">    grouped_city = datas_copy.groupby(datas[<span class="string">'工作地点'</span>])</span><br><span class="line">    grouped_city_count = grouped_city[<span class="string">'工作地点'</span>].agg([<span class="string">'count'</span>])    <span class="comment"># 对城市数量进行统计</span></span><br><span class="line">    grouped_city_count.reset_index(inplace = <span class="literal">True</span>)                  <span class="comment"># 若不进行此操作，将会报错 </span></span><br><span class="line">    city_data = [(grouped_city_count[<span class="string">'工作地点'</span>][i], grouped_city_count[<span class="string">'count'</span>][i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(grouped_city_count.shape[<span class="number">0</span>])]</span><br><span class="line">    <span class="comment">#print(city_data)</span></span><br><span class="line">    attr = []</span><br><span class="line">    value = []</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> city_data:</span><br><span class="line">        attrSub = re.findall(<span class="string">r'[(](.*?)[)]'</span>, str(obj))[<span class="number">0</span>].split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">        attr.append(attrSub.split(<span class="string">"'"</span>)[<span class="number">1</span>])</span><br><span class="line">        value.append(int(re.findall(<span class="string">r'[(](.*?)[)]'</span>, str(obj))[<span class="number">0</span>].split(<span class="string">','</span>)[<span class="number">1</span>].strip()))</span><br><span class="line">    <span class="comment">#print(attr)</span></span><br><span class="line">    <span class="comment">#print(value)</span></span><br><span class="line">    <span class="comment"># 全国工作分布热力图</span></span><br><span class="line">    geo = Geo()</span><br><span class="line">    geo = (</span><br><span class="line">        Geo()</span><br><span class="line">        .add_schema(maptype=<span class="string">"china"</span>)</span><br><span class="line">        .add(<span class="string">"岗位数"</span>,[list(z) <span class="keyword">for</span> z <span class="keyword">in</span> zip(attr, value)],symbol_size = <span class="number">20</span>)</span><br><span class="line">        .set_series_opts(label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>)) <span class="comment">#设置图例不可见</span></span><br><span class="line">        .set_global_opts(visualmap_opts=opts.VisualMapOpts(min_=<span class="number">0</span>,max_=<span class="number">40</span>),title_opts=opts.TitleOpts(title=<span class="string">"自动驾驶职位招聘地理位置"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">#geo.render('自动驾驶岗位全国热力图.html')</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全国招聘职位top10地区</span></span><br><span class="line">    city_top10 = sorted(city_data, key = <span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse = <span class="literal">True</span>)[<span class="number">0</span>:<span class="number">10</span>]</span><br><span class="line">    attr = []</span><br><span class="line">    value = []</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> city_top10:</span><br><span class="line">        attrSub = re.findall(<span class="string">r'[(](.*?)[)]'</span>, str(obj))[<span class="number">0</span>].split(<span class="string">','</span>)[<span class="number">0</span>]</span><br><span class="line">        attr.append(attrSub.split(<span class="string">"'"</span>)[<span class="number">1</span>])</span><br><span class="line">        value.append(int(re.findall(<span class="string">r'[(](.*?)[)]'</span>, str(obj))[<span class="number">0</span>].split(<span class="string">','</span>)[<span class="number">1</span>].strip()))</span><br><span class="line">    bar = (</span><br><span class="line">        Bar()</span><br><span class="line">        .add_xaxis(attr)</span><br><span class="line">        .add_yaxis(<span class="string">"招聘数量"</span>,value)</span><br><span class="line">        .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"自动驾驶岗位分布柱状图"</span>))</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#bar.render()</span></span><br><span class="line">    <span class="keyword">return</span> geo,bar</span><br><span class="line"></span><br><span class="line">geo,bar = city_counter()</span><br><span class="line">geo.render(<span class="string">'自动驾驶岗位全国热力图.html'</span>)</span><br><span class="line">bar.render(<span class="string">'自动驾驶岗位分布柱状图.html'</span>)</span><br><span class="line"><span class="comment">#make_snapshot(snapshot, geo.render(), "自动驾驶岗位全国热力图.png") # 使用此语句可直接生成图片</span></span><br><span class="line"><span class="comment">#make_snapshot(snapshot, bar.render(), "自动驾驶岗位分布柱状图.png") # 使用此语句可直接生成图片</span></span><br></pre></td></tr></tbody></table></figure><p>  从图中可以看出“自动驾驶”岗位主要分布在北京、上海、深圳、武汉、苏州、杭州等一线、二线城市，北京、江浙沪（包邮区）和深圳是主要阵地，如果想要从事这个领域的工作，到这些城市去发展有好的环境。</p><h3 id="2-2-薪资水平"><a href="#2-2-薪资水平" class="headerlink" title="2.2 薪资水平"></a>2.2 薪资水平</h3><p>  找工作更重要的是薪资的水平，辛辛苦苦学了很多技能和知识就是为了获得更多的薪资。所以我们来看一下“自动驾驶”岗位的全国范围的薪资水平是怎样的。本文对获取的数据按照城市分类，计算了各个城市的平均薪资水平，如图2-3所示。</p><img title="图2-3  自动驾驶全国平均薪资水平统计" data-src="/2019/10/17/BOSS直聘爬虫/图2-3%20%20自动驾驶全国平均薪资水平统计.png"><center>图2-3  自动驾驶全国平均薪资水平统计</center><p>  从图中可以看出，杭州以35K的平均月薪位居全国第一，全国范围内的平均薪资为18422元/月，佛山最低仅有5000元/月。月薪超过全国平均水平的数量超总体的60%。可以看出，薪资水平最高的城市在杭州、北京、上海、深圳等一线城市，杭州的物价水平和房价低于北上广等城市，落户和生活的成本与北上广相比较低，是一个不错的选择。</p><p>代码实现</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Bar</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyecharts.render <span class="keyword">import</span> make_snapshot</span><br><span class="line"><span class="keyword">from</span> snapshot_selenium <span class="keyword">import</span> snapshot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">deal_salary</span><span class="params">(salary_data)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> re.match(<span class="string">'.*K'</span>,salary_data):</span><br><span class="line">        <span class="comment"># 计算平均工资</span></span><br><span class="line">        <span class="keyword">return</span> int(float((float(re.findall(<span class="string">r'(.*?)K'</span>,salary_data)[<span class="number">0</span>].split(<span class="string">'-'</span>)[<span class="number">0</span>])*<span class="number">1000</span>+\</span><br><span class="line">            float(re.findall(<span class="string">r'(.*?)K'</span>,salary_data)[<span class="number">0</span>].split(<span class="string">'-'</span>)[<span class="number">1</span>])*<span class="number">1000</span>)/<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">city_salary</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">每个城市的平均工资条形统计图</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    datas_copy[<span class="string">'工作地点'</span>] = datas_copy[<span class="string">'工作地点'</span>].apply(<span class="keyword">lambda</span> x :x.split(<span class="string">' '</span>)[<span class="number">0</span>])</span><br><span class="line">    datas_copy = datas_copy[~datas_copy[<span class="string">'工资'</span>].str.contains(<span class="string">'天'</span>)] <span class="comment"># 除去实习的数据</span></span><br><span class="line">    datas_copy = datas_copy[~datas_copy[<span class="string">'工资'</span>].isna()] <span class="comment"># 除去工资为空的值</span></span><br><span class="line">    datas_copy[<span class="string">'工资'</span>] = datas_copy[<span class="string">'工资'</span>].apply(<span class="keyword">lambda</span> x :deal_salary(x))</span><br><span class="line">    grouped_city_salary = datas_copy[<span class="string">'工资'</span>].groupby(datas_copy[<span class="string">'工作地点'</span>])</span><br><span class="line">    salary_month = grouped_city_salary.agg([<span class="string">'mean'</span>]) <span class="comment">#计算各个城市的平均工资</span></span><br><span class="line">    salary_month.reset_index(inplace = <span class="literal">True</span>)</span><br><span class="line">    attr = []</span><br><span class="line">    value = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(salary_month.shape[<span class="number">0</span>]):</span><br><span class="line">        attr.append(salary_month[<span class="string">'工作地点'</span>][i])</span><br><span class="line">        value.append(str(salary_month[<span class="string">'mean'</span>][i])) <span class="comment">#将 int 数字转化为 string才能显示图片</span></span><br><span class="line">    bar = (</span><br><span class="line">        Bar()</span><br><span class="line">        .add_xaxis(attr)</span><br><span class="line">        .add_yaxis(<span class="string">"工资(元)"</span>,value)</span><br><span class="line">        .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"城市平均工资统计图"</span>))</span><br><span class="line">        .set_series_opts(</span><br><span class="line">            label_opts=opts.LabelOpts(is_show=<span class="literal">False</span>),</span><br><span class="line">            markline_opts=opts.MarkLineOpts(</span><br><span class="line">                data=[</span><br><span class="line">                    opts.MarkLineItem(type_=<span class="string">"average"</span>, name=<span class="string">"平均值"</span>)</span><br><span class="line">                ]</span><br><span class="line">            ),</span><br><span class="line">            markpoint_opts=opts.MarkPointOpts(</span><br><span class="line">                data=[</span><br><span class="line">                    opts.MarkPointItem(type_=<span class="string">"max"</span>, name=<span class="string">"最大值"</span>),</span><br><span class="line">                    opts.MarkPointItem(type_=<span class="string">"min"</span>, name=<span class="string">"最小值"</span>),</span><br><span class="line">                ]</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> bar</span><br><span class="line"></span><br><span class="line">city_mean_salary = city_salary()</span><br><span class="line">city_mean_salary.render()</span><br><span class="line"><span class="comment">#make_snapshot(snapshot, city_mean_salary.render(), "城市平均工资统计图.png") # 使用此语句可直接生成图片</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-3-学历、工作经验要求"><a href="#2-3-学历、工作经验要求" class="headerlink" title="2.3 学历、工作经验要求"></a>2.3 学历、工作经验要求</h3><p>  你可能会问这些岗位这么好，会对学历有很高的要求吗？（学历在不同行业都是越高越好吗？）对工作经验又有什么样的要求呢？那么我们就通过数据来看一下结果吧，通过对所获取信息中的学历要求、工作经验进行统计，绘制“自动驾驶”职位学历要求饼图、工作经验要求饼图如图2-4和图2-5所示。</p><img title="图2-4  “自动驾驶”职位学历要求" data-src="/2019/10/17/BOSS直聘爬虫/图2-4%20%20“自动驾驶”职位学历要求.png"><center>图2-4  “自动驾驶”职位学历要求</center><img title="图2-5  “自动驾驶”职位工作经验要求" data-src="/2019/10/17/BOSS直聘爬虫/图2-5%20%20“自动驾驶”职位工作经验要求.png"><center>图2-5  “自动驾驶”职位工作经验要求</center><p>  通过统计结果可以看出，在获取的岗位招聘信息中，自动驾驶相关岗位对学历的要求不是很高，本科学历要求占75%以上，硕士学历要求占15%左右。结合经验要求来看，大部分的公司要求是经验不限和1-5年，这几个时间范围占了主要部分，所以结合这两个信息，认为“自动驾驶”岗位更加看重工作经验，只要有本科的学历，经验丰富能力强就可以找到不错的工作。</p><p>代码实现</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Page, Pie</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> pyecharts.render <span class="keyword">import</span> make_snapshot</span><br><span class="line"><span class="keyword">from</span> snapshot_selenium <span class="keyword">import</span> snapshot</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">edu_require</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">学历要求饼图</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    datas_copy = datas_copy[~datas_copy[<span class="string">'学历要求'</span>].str.contains(<span class="string">'个月'</span>)]   <span class="comment"># 去掉实习工作</span></span><br><span class="line">    grouped_eductaion = datas_copy.groupby(datas_copy[<span class="string">'学历要求'</span>]) <span class="comment"># 按学历分组</span></span><br><span class="line">    grouped_eductaion_count = grouped_eductaion[<span class="string">'学历要求'</span>].agg([<span class="string">'count'</span>]) <span class="comment"># 统计不同分组的数量</span></span><br><span class="line">    grouped_eductaion_count.reset_index(inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># edu_data = [(grouped_eductaion_count['学历要求'][i], grouped_eductaion_count['count'][i]) for i in range(grouped_eductaion_count.shape[0])]</span></span><br><span class="line">    attr = []</span><br><span class="line">    value = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(grouped_eductaion_count.shape[<span class="number">0</span>]):</span><br><span class="line">        attr.append(grouped_eductaion_count[<span class="string">'学历要求'</span>][i])</span><br><span class="line">        value.append(str(grouped_eductaion_count[<span class="string">'count'</span>][i])) <span class="comment">#将 int 数字转化为 string才能显示图片</span></span><br><span class="line">    pie = (</span><br><span class="line">        Pie()</span><br><span class="line">        .add(</span><br><span class="line">            <span class="string">""</span>,</span><br><span class="line">            [list(z) <span class="keyword">for</span> z <span class="keyword">in</span> zip(attr, value)],</span><br><span class="line">            radius=[<span class="string">"40%"</span>, <span class="string">"75%"</span>]</span><br><span class="line">        )</span><br><span class="line">        .set_global_opts(</span><br><span class="line">            title_opts=opts.TitleOpts(title=<span class="string">"学历要求"</span>),</span><br><span class="line">            legend_opts=opts.LegendOpts(</span><br><span class="line">                orient=<span class="string">"vertical"</span>, pos_top=<span class="string">"15%"</span>, pos_left=<span class="string">"2%"</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        .set_series_opts(label_opts=opts.LabelOpts(formatter=<span class="string">"{b}: {c}"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> pie</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exp_require</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">工作经验要求饼图</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    datas_copy = datas_copy[~datas_copy[<span class="string">'工作经验要求'</span>].str.contains(<span class="string">'天/周'</span>)]   <span class="comment"># 去掉实习工作</span></span><br><span class="line">    grouped_exp = datas_copy.groupby(datas_copy[<span class="string">'工作经验要求'</span>]) <span class="comment"># 按工作经验分组</span></span><br><span class="line">    grouped_exp_count = grouped_exp[<span class="string">'工作经验要求'</span>].agg([<span class="string">'count'</span>]) <span class="comment"># 统计不同分组的数量</span></span><br><span class="line">    grouped_exp_count.reset_index(inplace = <span class="literal">True</span>)</span><br><span class="line">    attr = []</span><br><span class="line">    value = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(grouped_exp_count.shape[<span class="number">0</span>]):</span><br><span class="line">        attr.append(grouped_exp_count[<span class="string">'工作经验要求'</span>][i])</span><br><span class="line">        value.append(str(grouped_exp_count[<span class="string">'count'</span>][i])) <span class="comment">#将 int 数字转化为 string才能显示图片</span></span><br><span class="line">    pie = (</span><br><span class="line">        Pie()</span><br><span class="line">        .add(</span><br><span class="line">            <span class="string">""</span>,</span><br><span class="line">            [list(z) <span class="keyword">for</span> z <span class="keyword">in</span> zip(attr, value)],</span><br><span class="line">            radius=[<span class="string">"40%"</span>, <span class="string">"75%"</span>]</span><br><span class="line">        )</span><br><span class="line">        .set_global_opts(</span><br><span class="line">            title_opts=opts.TitleOpts(title=<span class="string">"工作经验要求"</span>),</span><br><span class="line">            legend_opts=opts.LegendOpts(</span><br><span class="line">                orient=<span class="string">"vertical"</span>, pos_top=<span class="string">"15%"</span>, pos_left=<span class="string">"2%"</span></span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        .set_series_opts(label_opts=opts.LabelOpts(formatter=<span class="string">"{b}: {c}"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> pie</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">edu_pie = edu_require()</span><br><span class="line">edu_pie.render(<span class="string">"学历要求饼图.html"</span>)</span><br><span class="line">exp_pie = exp_require()</span><br><span class="line">exp_pie.render(<span class="string">"工作经验要求饼图.html"</span>)</span><br><span class="line"><span class="comment">#make_snapshot(snapshot, edu_pie.render(), "学历要求饼图.png") # 使用此语句可直接生成图片</span></span><br><span class="line"><span class="comment">#make_snapshot(snapshot, exp_pie.render(), "工作经验要求饼图.png") # 使用此语句可直接生成图片</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-工作种类、主要企业、行业分布"><a href="#2-4-工作种类、主要企业、行业分布" class="headerlink" title="2.4 工作种类、主要企业、行业分布"></a>2.4 工作种类、主要企业、行业分布</h3><p>  分析完学历与工作经验，我们来分析一下这些工作主要包括哪些种类，也就是说“自动驾驶”的招聘岗位主要包括哪些，可以为求职者提供学习的方向。为了更好地展示数据分析的成果，我们使用了优秀的开源包<span class="exturl" data-url="aHR0cHM6Ly9weXBpLm9yZy9wcm9qZWN0L2ppZWJhLw==" title="https://pypi.org/project/jieba/">jieba<i class="fa fa-external-link"></i></span>以及强大、方便的可视化工具<span class="exturl" data-url="aHR0cHM6Ly9weXBpLm9yZy9wcm9qZWN0L3B5ZWNoYXJ0cy8=" title="https://pypi.org/project/pyecharts/">pyecharts<i class="fa fa-external-link"></i></span>展示，根据“工作名”这一项数据项先进行分词，然后绘制了词云如图2-6所示。</p><img title="图2-6  “自动驾驶”主要岗位种类分析" data-src="/2019/10/17/BOSS直聘爬虫/图2-6%20%20“自动驾驶”主要岗位种类分析.png"><center>图2-6  “自动驾驶”主要岗位种类分析</center><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Page, WordCloud</span><br><span class="line"><span class="keyword">from</span> pyecharts.globals <span class="keyword">import</span> SymbolType</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">job_title</span><span class="params">()</span>:</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    text = <span class="string">''</span>.join(datas_copy[<span class="string">'岗位'</span>])</span><br><span class="line">    requirements = [word <span class="keyword">for</span> word <span class="keyword">in</span> jieba.cut(text, cut_all=<span class="literal">True</span>)] <span class="comment"># 全模式分词</span></span><br><span class="line">    requirements_top = Counter(requirements)  <span class="comment"># Counter()函数对str进行统计</span></span><br><span class="line">    requirements_top50 = requirements_top.most_common(<span class="number">50</span>)    <span class="comment"># 统计最多的50个 ，most_common返回(string , count)的数据形式</span></span><br><span class="line">    c = (</span><br><span class="line">        WordCloud()</span><br><span class="line">        .add(<span class="string">""</span>, requirements_top50, word_size_range=[<span class="number">20</span>, <span class="number">100</span>], shape=SymbolType.DIAMOND)</span><br><span class="line">        .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"主要岗位名称"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">job_title = job_title()</span><br><span class="line">job_title.render(<span class="string">"主要岗位名称.html"</span>)</span><br></pre></td></tr></tbody></table></figure><p>  从图中可以看出，<code>算法</code>、<code>规划</code>、<code>感知</code>、<code>控制</code>、<code>嵌入式</code>等关键词频率较高，所以想要从事这一领域的工作，可以多多关注和学习这些领域的知识。为以后的求职做好准备，打下基础。<br>  那么哪些公司是招聘的大户呢？即招聘岗位数量较多的企业有哪些？我们使用同样的方法绘制了基于企业招聘数量的词云如图2-7所示。</p><img title="图2-7  “自动驾驶”主要招聘企业" data-src="/2019/10/17/BOSS直聘爬虫/图2-7%20%20“自动驾驶”主要招聘企业.png"><center>图2-7  “自动驾驶”主要招聘企业</center><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Page, WordCloud</span><br><span class="line"><span class="keyword">from</span> pyecharts.globals <span class="keyword">import</span> SymbolType</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">company_count</span><span class="params">()</span>:</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    grouped_company = datas_copy.groupby(datas_copy[<span class="string">'招聘公司'</span>]) <span class="comment"># 按招聘公司分组</span></span><br><span class="line">    grouped_company_count = grouped_company[<span class="string">'招聘公司'</span>].agg([<span class="string">'count'</span>]) <span class="comment"># 统计不同分组的数量</span></span><br><span class="line">    grouped_company_count.reset_index(inplace = <span class="literal">True</span>)</span><br><span class="line">    company_data = [(grouped_company_count[<span class="string">'招聘公司'</span>][i],str(grouped_company_count[<span class="string">'count'</span>][i])) \</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(grouped_company_count.shape[<span class="number">0</span>])]</span><br><span class="line">    <span class="comment"># 是否要将数字转化为str</span></span><br><span class="line"></span><br><span class="line">    c = (</span><br><span class="line">        WordCloud()</span><br><span class="line">        .add(<span class="string">""</span>, company_data, word_size_range=[<span class="number">20</span>, <span class="number">100</span>], shape=SymbolType.DIAMOND)</span><br><span class="line">        .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"主要的招聘公司"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">company_count = company_count()</span><br><span class="line">company_count.render(<span class="string">"主要的招聘公司.html"</span>)</span><br><span class="line"><span class="comment">#make_snapshot(snapshot, company_count.render(), "主要的招聘公司.png") # 使用此语句可直接生成图片</span></span><br></pre></td></tr></tbody></table></figure><p>  可以看出，招聘数量排在前几位的分别有禾多科技、<span class="exturl" data-url="aHR0cHM6Ly93d3cuZGVlcGJsdWVhaS5jb20=" title="https://www.deepblueai.com">深兰科技<i class="fa fa-external-link"></i></span>、<span class="exturl" data-url="aHR0cHM6Ly93d3cuY3Rpcm9ib3QuY29t" title="https://www.ctirobot.com">坎德拉<i class="fa fa-external-link"></i></span>、<span class="exturl" data-url="aHR0cDovL3d3dy41MWhpdGVjaC5jb20=" title="http://www.51hitech.com">51VR<i class="fa fa-external-link"></i></span>、<span class="exturl" data-url="aHR0cDovL3d3dy5rb3RlaS5jb20uY24=" title="http://www.kotei.com.cn">武汉光庭科技<i class="fa fa-external-link"></i></span>、<span class="exturl" data-url="aHR0cDovL3d3dy55aWhhbmcuYWk=" title="http://www.yihang.ai">易航<i class="fa fa-external-link"></i></span>等公司，互联网大厂腾讯、华为，传统车厂吉利集团紧随其后，还有很多年轻的企业也有上榜。</p><p>  这些岗位所属的行业又有哪些呢？我们使用同样的方法统计如图2-8所示。</p><img title="图2-8  自动驾驶招聘行业分布" data-src="/2019/10/17/BOSS直聘爬虫/图2-8%20%20自动驾驶招聘行业分布.png"><center>图2-8  自动驾驶招聘行业分布</center><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> pyecharts <span class="keyword">import</span> options <span class="keyword">as</span> opts</span><br><span class="line"><span class="keyword">from</span> pyecharts.charts <span class="keyword">import</span> Page, WordCloud</span><br><span class="line"><span class="keyword">from</span> pyecharts.globals <span class="keyword">import</span> SymbolType</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">industry</span><span class="params">()</span>:</span></span><br><span class="line">    datas = pd.read_excel(<span class="string">'jobData.xls'</span>, encoding = <span class="string">'utf-8'</span>)</span><br><span class="line">    datas_copy = datas</span><br><span class="line">    grouped_industry = datas_copy.groupby(datas_copy[<span class="string">'所属行业'</span>]) <span class="comment"># 按所属行业分组</span></span><br><span class="line">    grouped_industry_count = grouped_industry[<span class="string">'所属行业'</span>].agg([<span class="string">'count'</span>]) <span class="comment"># 统计不同分组的数量</span></span><br><span class="line">    grouped_industry_count.reset_index(inplace = <span class="literal">True</span>)</span><br><span class="line">    industry_data = [(grouped_industry_count[<span class="string">'所属行业'</span>][i],str(grouped_industry_count[<span class="string">'count'</span>][i])) \</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(grouped_industry_count.shape[<span class="number">0</span>])]</span><br><span class="line">    c = (</span><br><span class="line">        WordCloud()</span><br><span class="line">        .add(<span class="string">""</span>, industry_data, word_size_range=[<span class="number">20</span>, <span class="number">100</span>], shape=SymbolType.DIAMOND)</span><br><span class="line">        .set_global_opts(title_opts=opts.TitleOpts(title=<span class="string">"自动驾驶岗位行业分布"</span>))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line">industry_distribution = industry()</span><br><span class="line">industry_distribution.render(<span class="string">"自动驾驶岗位行业分布.html"</span>)</span><br><span class="line"><span class="comment">#make_snapshot(snapshot, industry_distribution.render(), "自动驾驶岗位行业分布.png") # 使用此语句可直接生成图片</span></span><br></pre></td></tr></tbody></table></figure><p>  从图中可以看出现在“自动驾驶”的企业所示行业多为“计算机互联网”、“智能硬件”以及“汽车生产”行业，这三类行业是招聘的主力军。</p><h2 id="三、总结与讨论"><a href="#三、总结与讨论" class="headerlink" title="三、总结与讨论"></a>三、总结与讨论</h2><h3 id="3-1-总结"><a href="#3-1-总结" class="headerlink" title="3.1 总结"></a>3.1 总结</h3><p>从两个方面进行总结，</p><ul><li><p>首先是内容上：从数据的分析中对于<strong>自动驾驶</strong>岗位的招聘需求有了进一步的认识，自动驾驶相关岗位主要的工作内容有<strong>算法、规划、感知、控制、嵌入式</strong>，北京、江浙沪（包邮区）和深圳是主要的工作地点，全国范围内的平均薪资为18422元/月，自动驾驶岗位更加看重工作经验、学历多数要求为本科，所在行业一般为<strong>计算机互联网、智能硬件以及汽车生产</strong>，招聘需求较多的企业有禾多科技、深兰科技、坎德拉、51VR、武汉光庭科技、易航等公司。</p></li><li><p>其次是技术上总结：<br>在数据获取中学会使用开发者工具发现网页规律；在数据解析中学会使用<code>xpath</code>、<code>beautifulSoup</code>；数据分析中学会使用<code>pyecharts</code>绘制多种有趣的可视化图片，学会使用<code>jieba</code>包统计词频。</p></li></ul><h3 id="3-2-遇到的问题"><a href="#3-2-遇到的问题" class="headerlink" title="3.2 遇到的问题"></a>3.2 遇到的问题</h3><p>  本文一开始想要获取更详细的<code>工作职责</code>和<code>工作技能要求</code>的数据，在爬取详情页的过程中发现BOSS直聘在请求详情页时每次的请求都会产生不同的<code>cookie</code>，否则无法获取想要的html，会跳转到其他页面，经过google查询已经有前辈发现了这个问题，但是还未找到解决的反反爬方法（魔高一尺道高一丈）。所以改为爬取较为容易获取的详情页面。</p><h3 id="3-3-不足之处"><a href="#3-3-不足之处" class="headerlink" title="3.3 不足之处"></a>3.3 不足之处</h3><p>  本文仅仅分析了BOSS直聘一家的数据，数据来源与实际情况有一定的差别，但是能体现一定的代表性，未来可以进行多家招聘网站（如<strong>智联招聘、51job招聘，前程无忧</strong>等）的数据爬取，提高数据的可信度。</p><h3 id="3-4-未来工作"><a href="#3-4-未来工作" class="headerlink" title="3.4 未来工作"></a>3.4 未来工作</h3><p>  目前已经完成数据的<code>获取、分析、统计、可视化</code>，未来还可以对数据之间的<code>相关性</code>进行<code>建模分析</code>，得出有意思、有价值的结论。（例如，哪些数据对薪资有影响？）</p><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>  <em>本文的主要方法与技术代码参考了网络上的博客，在此一并致谢！</em></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]    LI D, MEI H, YI S, et al. ECharts: A declarative framework for rapid construction of web-based visualization ☆ [J]. Visual Informatics, 2018, S2468502X18300068-.</p><p>[2] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQyMjkzNzU4L2FydGljbGUvZGV0YWlscy84ODU3MzM2MA==" title="https://blog.csdn.net/qq_42293758/article/details/88573360">Python爬取岗位数据并分析<i class="fa fa-external-link"></i></span></p><p>[3] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzkzMDY5NC9hcnRpY2xlL2RldGFpbHMvOTY2NTAyODE=" title="https://blog.csdn.net/weixin_43930694/article/details/96650281">Python3 + xpath + excel 实现对boss直聘网的爬取<i class="fa fa-external-link"></i></span></p><p>[4] <span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI3NjY4MzEzL2FydGljbGUvZGV0YWlscy84MjkyNDYyMg==" title="https://blog.csdn.net/qq_27668313/article/details/82924622">BOSS直聘网站数据分析岗位信息爬取<i class="fa fa-external-link"></i></span></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  这是我个人博客的第一篇文章，是全新的开始。希望未来在这里记录更多有趣的探索。&lt;/p&gt;
&lt;img title=&quot;自动驾驶岗位名称&quot; data-src=&quot;/2019/10/17/BOSS直聘爬虫/岗位名称.jpg&quot;&gt;
&lt;p&gt;  自动驾驶随着人工智能技术的成熟，人才需求在不断增加，为了对我国自动驾驶人才需求进行了解，本文选择&lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly93d3cuemhpcGluLmNvbQ==&quot; title=&quot;https://www.zhipin.com&quot;&gt;BOSS直聘&lt;i class=&quot;fa fa-external-link&quot;&gt;&lt;/i&gt;&lt;/span&gt;招聘网站获取自动驾驶相关职位信息，分析数据，根据分析结果确定人才需求、为以后的学习制定计划。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="2019秋《交通数据分析》" scheme="http://www.caoxu.club/categories/2019%E7%A7%8B%E3%80%8A%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B/"/>
    
      <category term="数据爬取" scheme="http://www.caoxu.club/categories/2019%E7%A7%8B%E3%80%8A%E4%BA%A4%E9%80%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E3%80%8B/%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/"/>
    
    
      <category term="python" scheme="http://www.caoxu.club/tags/python/"/>
    
      <category term="爬虫" scheme="http://www.caoxu.club/tags/%E7%88%AC%E8%99%AB/"/>
    
      <category term="工作信息" scheme="http://www.caoxu.club/tags/%E5%B7%A5%E4%BD%9C%E4%BF%A1%E6%81%AF/"/>
    
      <category term="可视化" scheme="http://www.caoxu.club/tags/%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="pyecharts" scheme="http://www.caoxu.club/tags/pyecharts/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://www.caoxu.club/2019/10/12/hello-world/"/>
    <id>http://www.caoxu.club/2019/10/12/hello-world/</id>
    <published>2019-10-12T13:25:37.410Z</published>
    <updated>2019-10-14T16:39:38.421Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvLw==" title="https://hexo.io/">Hexo<i class="fa fa-external-link"></i></span>! This is your very first post. Check <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mv" title="https://hexo.io/docs/">documentation<i class="fa fa-external-link"></i></span> for more info. If you get any problems when using Hexo, you can find the answer in <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvdHJvdWJsZXNob290aW5nLmh0bWw=" title="https://hexo.io/docs/troubleshooting.html">troubleshooting<i class="fa fa-external-link"></i></span> or you can ask me on <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2hleG9qcy9oZXhvL2lzc3Vlcw==" title="https://github.com/hexojs/hexo/issues">GitHub<i class="fa fa-external-link"></i></span>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvd3JpdGluZy5odG1s" title="https://hexo.io/docs/writing.html">Writing<i class="fa fa-external-link"></i></span></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvc2VydmVyLmh0bWw=" title="https://hexo.io/docs/server.html">Server<i class="fa fa-external-link"></i></span></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvZ2VuZXJhdGluZy5odG1s" title="https://hexo.io/docs/generating.html">Generating<i class="fa fa-external-link"></i></span></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>More info: <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlvL2RvY3MvZGVwbG95bWVudC5odG1s" title="https://hexo.io/docs/deployment.html">Deployment<i class="fa fa-external-link"></i></span></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;span class=&quot;exturl&quot; data-url=&quot;aHR0cHM6Ly9oZXhvLmlvLw==&quot; title=&quot;https://hexo.io/&quot;&gt;Hexo&lt;i class=&quot;fa fa-external-link&quot;&gt;&lt;/i&gt;&lt;/spa
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
